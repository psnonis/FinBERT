{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(ckpt, conf = '/gold/GooBERT/bert_config.json'):\n",
    "    \n",
    "    from os.path import basename, dirname\n",
    "    \n",
    "    bin = f'{basename(dirname(ckpt))}.bin'\n",
    "    \n",
    "    !pytorch_transformers bert {ckpt} {conf} {bin}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building PyTorch model from configuration: {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0801 02:08:12.067471 140230207002432 modeling_bert.py:84] Converting TensorFlow checkpoint from /root/w266-final/pred/gold/FinBERT-Combo_128MSL-250K/model.ckpt-250000\n",
      "I0801 02:08:12.624076 140230207002432 modeling_bert.py:90] Loading TF weight bad_steps with shape []\n",
      "I0801 02:08:13.453151 140230207002432 modeling_bert.py:90] Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n",
      "I0801 02:08:14.336717 140230207002432 modeling_bert.py:90] Loading TF weight bert/embeddings/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:08:15.267876 140230207002432 modeling_bert.py:90] Loading TF weight bert/embeddings/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:08:16.072289 140230207002432 modeling_bert.py:90] Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n",
      "I0801 02:08:16.875921 140230207002432 modeling_bert.py:90] Loading TF weight bert/embeddings/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:08:17.554159 140230207002432 modeling_bert.py:90] Loading TF weight bert/embeddings/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:08:18.314183 140230207002432 modeling_bert.py:90] Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n",
      "I0801 02:08:19.071114 140230207002432 modeling_bert.py:90] Loading TF weight bert/embeddings/position_embeddings/adam_m with shape [512, 768]\n",
      "I0801 02:08:19.756304 140230207002432 modeling_bert.py:90] Loading TF weight bert/embeddings/position_embeddings/adam_v with shape [512, 768]\n",
      "I0801 02:08:20.640018 140230207002432 modeling_bert.py:90] Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n",
      "I0801 02:08:21.480338 140230207002432 modeling_bert.py:90] Loading TF weight bert/embeddings/token_type_embeddings/adam_m with shape [2, 768]\n",
      "I0801 02:08:22.166900 140230207002432 modeling_bert.py:90] Loading TF weight bert/embeddings/token_type_embeddings/adam_v with shape [2, 768]\n",
      "I0801 02:08:22.985937 140230207002432 modeling_bert.py:90] Loading TF weight bert/embeddings/word_embeddings with shape [30522, 768]\n",
      "I0801 02:08:31.221020 140230207002432 modeling_bert.py:90] Loading TF weight bert/embeddings/word_embeddings/adam_m with shape [30522, 768]\n",
      "I0801 02:08:39.572771 140230207002432 modeling_bert.py:90] Loading TF weight bert/embeddings/word_embeddings/adam_v with shape [30522, 768]\n",
      "I0801 02:08:47.762112 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:08:48.566068 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:08:49.321940 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:08:50.045690 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:08:50.849701 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:08:51.693684 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:08:52.416919 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
      "I0801 02:08:53.297163 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:08:54.104866 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:08:54.789172 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
      "I0801 02:08:55.686259 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:08:56.633262 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:08:57.370077 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
      "I0801 02:08:58.162967 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/key/bias/adam_m with shape [768]\n",
      "I0801 02:08:58.849924 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/key/bias/adam_v with shape [768]\n",
      "I0801 02:08:59.705617 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
      "I0801 02:09:00.689071 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:09:01.466882 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:09:02.416977 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
      "I0801 02:09:03.229174 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/query/bias/adam_m with shape [768]\n",
      "I0801 02:09:03.956611 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/query/bias/adam_v with shape [768]\n",
      "I0801 02:09:04.800559 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
      "I0801 02:09:05.630500 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:09:06.500885 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:09:07.358908 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
      "I0801 02:09:08.166695 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/value/bias/adam_m with shape [768]\n",
      "I0801 02:09:08.978156 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/value/bias/adam_v with shape [768]\n",
      "I0801 02:09:09.822384 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
      "I0801 02:09:10.768537 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:09:11.549663 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:09:12.456907 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
      "I0801 02:09:13.269530 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0801 02:09:13.953499 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0801 02:09:14.836814 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0801 02:09:16.316461 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0801 02:09:17.843908 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0801 02:09:19.318416 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:09:20.239145 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:09:21.002037 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:09:21.765229 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:09:22.569273 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:09:23.413002 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:09:24.100756 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n",
      "I0801 02:09:24.944763 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:09:25.708996 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:09:26.472420 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
      "I0801 02:09:27.954928 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0801 02:09:29.436676 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0801 02:09:30.954392 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:09:31.750248 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:09:32.386226 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:09:33.178120 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:09:33.814266 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:09:34.578820 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:09:35.374191 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
      "I0801 02:09:36.087665 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:09:36.882781 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:09:37.594429 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
      "I0801 02:09:38.465676 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:09:39.412622 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:09:40.149780 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
      "I0801 02:09:40.993790 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/key/bias/adam_m with shape [768]\n",
      "I0801 02:09:41.797505 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/key/bias/adam_v with shape [768]\n",
      "I0801 02:09:42.537096 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
      "I0801 02:09:43.483447 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:09:44.378034 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:09:45.204411 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
      "I0801 02:09:46.008696 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/query/bias/adam_m with shape [768]\n",
      "I0801 02:09:46.852141 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/query/bias/adam_v with shape [768]\n",
      "I0801 02:09:47.530289 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
      "I0801 02:09:48.385372 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:09:49.332425 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:09:50.029888 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
      "I0801 02:09:50.913231 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/value/bias/adam_m with shape [768]\n",
      "I0801 02:09:51.717154 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/value/bias/adam_v with shape [768]\n",
      "I0801 02:09:52.440953 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
      "I0801 02:09:53.298257 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:09:54.329537 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:09:55.075207 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
      "I0801 02:09:55.961929 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0801 02:09:56.769970 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0801 02:09:57.493906 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0801 02:09:58.972994 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0801 02:10:00.504805 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0801 02:10:02.024540 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:10:02.948830 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:10:03.752872 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:10:04.466125 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:10:05.226039 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:10:05.902756 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:10:06.619122 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n",
      "I0801 02:10:07.294871 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:10:08.094069 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:10:08.906386 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
      "I0801 02:10:10.349931 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0801 02:10:11.909041 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0801 02:10:13.393045 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:10:14.240712 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:10:15.042332 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:10:15.726220 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:10:16.493927 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:10:17.135330 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:10:17.891026 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
      "I0801 02:10:18.722039 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:10:19.525917 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:10:20.370028 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
      "I0801 02:10:21.160942 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:10:22.058117 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:10:22.925559 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
      "I0801 02:10:23.812855 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/key/bias/adam_m with shape [768]\n",
      "I0801 02:10:24.616996 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/key/bias/adam_v with shape [768]\n",
      "I0801 02:10:25.460980 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
      "I0801 02:10:26.217929 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:10:27.164908 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:10:28.022326 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
      "I0801 02:10:28.746190 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/query/bias/adam_m with shape [768]\n",
      "I0801 02:10:29.561095 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/query/bias/adam_v with shape [768]\n",
      "I0801 02:10:30.482971 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
      "I0801 02:10:31.272550 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:10:32.178138 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:10:33.165260 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
      "I0801 02:10:33.892652 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/value/bias/adam_m with shape [768]\n",
      "I0801 02:10:34.696990 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/value/bias/adam_v with shape [768]\n",
      "I0801 02:10:35.544716 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
      "I0801 02:10:36.282091 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:10:37.228944 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:10:38.086361 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
      "I0801 02:10:38.814150 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0801 02:10:39.586454 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0801 02:10:40.430173 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0801 02:10:41.790007 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0801 02:10:43.348999 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0801 02:10:44.880594 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:10:45.724605 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:10:46.530667 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:10:47.216925 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:10:48.099762 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:10:48.950863 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:10:49.586790 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n",
      "I0801 02:10:50.418470 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:10:51.055630 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:10:51.811012 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
      "I0801 02:10:53.170544 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0801 02:10:54.701799 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0801 02:10:56.233594 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:10:57.077303 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:10:57.885091 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:10:58.732863 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:10:59.536724 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:11:00.380613 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:11:01.064250 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
      "I0801 02:11:01.868922 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:11:02.712623 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:11:03.386195 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
      "I0801 02:11:04.249793 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:11:05.232820 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:11:05.930319 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
      "I0801 02:11:06.774435 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/key/bias/adam_m with shape [768]\n",
      "I0801 02:11:07.578362 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/key/bias/adam_v with shape [768]\n",
      "I0801 02:11:08.301991 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
      "I0801 02:11:09.328687 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:11:10.266009 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:11:11.017499 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
      "I0801 02:11:11.904784 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/query/bias/adam_m with shape [768]\n",
      "I0801 02:11:12.708875 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/query/bias/adam_v with shape [768]\n",
      "I0801 02:11:13.436852 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
      "I0801 02:11:14.294518 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:11:15.241135 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:11:15.939192 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
      "I0801 02:11:16.782605 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/value/bias/adam_m with shape [768]\n",
      "I0801 02:11:17.534412 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/value/bias/adam_v with shape [768]\n",
      "I0801 02:11:18.210624 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
      "I0801 02:11:19.069949 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:11:20.056972 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:11:20.798228 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
      "I0801 02:11:21.642062 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0801 02:11:22.445894 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0801 02:11:23.170009 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0801 02:11:24.688725 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0801 02:11:26.226778 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0801 02:11:27.658600 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:11:28.541708 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:11:29.345990 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:11:30.189656 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:11:30.961798 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:11:31.845195 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:11:32.528815 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n",
      "I0801 02:11:33.372811 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:11:34.180917 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:11:34.904700 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
      "I0801 02:11:36.344835 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0801 02:11:37.943753 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0801 02:11:39.461970 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:11:40.305537 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:11:41.121324 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:11:41.858402 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:11:42.661116 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:11:43.465463 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:11:44.208558 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
      "I0801 02:11:45.052697 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:11:45.856332 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:11:46.530287 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
      "I0801 02:11:47.393549 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:11:48.356700 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:11:49.171441 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
      "I0801 02:11:49.974348 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/key/bias/adam_m with shape [768]\n",
      "I0801 02:11:50.782298 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/key/bias/adam_v with shape [768]\n",
      "I0801 02:11:51.545499 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
      "I0801 02:11:52.452674 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:11:53.349955 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:11:54.136857 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
      "I0801 02:11:54.981148 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/query/bias/adam_m with shape [768]\n",
      "I0801 02:11:55.824574 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/query/bias/adam_v with shape [768]\n",
      "I0801 02:11:56.548347 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
      "I0801 02:11:57.405704 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:11:58.352812 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:11:59.106449 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
      "I0801 02:12:00.070291 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/value/bias/adam_m with shape [768]\n",
      "I0801 02:12:00.885791 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/value/bias/adam_v with shape [768]\n",
      "I0801 02:12:01.617670 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
      "I0801 02:12:02.524224 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:12:03.421934 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:12:04.169063 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
      "I0801 02:12:05.036731 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0801 02:12:05.830497 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0801 02:12:06.506176 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0801 02:12:07.934413 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0801 02:12:09.457805 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0801 02:12:11.097255 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:12:11.941008 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:12:12.744782 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:12:13.468764 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:12:14.272758 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:12:15.120780 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:12:15.804508 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n",
      "I0801 02:12:16.606632 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:12:17.370682 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:12:18.051104 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
      "I0801 02:12:19.507971 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0801 02:12:21.106361 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0801 02:12:22.665366 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:12:23.510807 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:12:24.277545 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:12:25.041063 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:12:25.860933 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:12:26.781222 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:12:27.508475 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
      "I0801 02:12:28.352421 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:12:29.156482 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:12:29.791039 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
      "I0801 02:12:30.646102 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:12:31.512733 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:12:32.370596 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
      "I0801 02:12:33.182291 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/key/bias/adam_m with shape [768]\n",
      "I0801 02:12:33.866270 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/key/bias/adam_v with shape [768]\n",
      "I0801 02:12:34.749556 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
      "I0801 02:12:35.616815 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:12:36.437915 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:12:37.464538 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
      "I0801 02:12:38.308578 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/query/bias/adam_m with shape [768]\n",
      "I0801 02:12:38.954550 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/query/bias/adam_v with shape [768]\n",
      "I0801 02:12:39.746398 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
      "I0801 02:12:40.485439 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:12:41.393204 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:12:42.250352 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
      "I0801 02:12:43.054301 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/value/bias/adam_m with shape [768]\n",
      "I0801 02:12:43.857896 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/value/bias/adam_v with shape [768]\n",
      "I0801 02:12:44.701977 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
      "I0801 02:12:45.478763 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:12:46.338627 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:12:47.288620 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
      "I0801 02:12:48.012553 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0801 02:12:48.770439 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0801 02:12:49.407020 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0801 02:12:50.874100 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0801 02:12:52.393821 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0801 02:12:54.033350 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:12:54.876939 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:12:55.681025 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:12:56.405059 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:12:57.208861 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:12:58.060903 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:12:58.745079 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n",
      "I0801 02:12:59.592691 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:13:00.404547 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:13:01.039533 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
      "I0801 02:13:02.635405 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0801 02:13:04.158071 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0801 02:13:05.677606 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:13:06.525352 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:13:07.328916 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:13:08.053091 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:13:08.857136 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:13:09.701213 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:13:10.389147 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
      "I0801 02:13:11.232972 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:13:12.084218 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:13:12.758535 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
      "I0801 02:13:13.665959 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:13:14.625021 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:13:15.362419 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
      "I0801 02:13:16.206064 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/key/bias/adam_m with shape [768]\n",
      "I0801 02:13:16.974293 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/key/bias/adam_v with shape [768]\n",
      "I0801 02:13:17.698115 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
      "I0801 02:13:18.644301 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:13:19.541788 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:13:20.372381 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
      "I0801 02:13:21.256837 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/query/bias/adam_m with shape [768]\n",
      "I0801 02:13:22.012087 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/query/bias/adam_v with shape [768]\n",
      "I0801 02:13:22.686634 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
      "I0801 02:13:23.502514 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:13:24.372491 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:13:25.230202 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
      "I0801 02:13:26.073868 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/value/bias/adam_m with shape [768]\n",
      "I0801 02:13:26.757796 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/value/bias/adam_v with shape [768]\n",
      "I0801 02:13:27.601161 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
      "I0801 02:13:28.459060 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:13:29.254180 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:13:30.152072 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
      "I0801 02:13:31.045520 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0801 02:13:31.770384 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0801 02:13:32.613943 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0801 02:13:34.098699 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0801 02:13:35.616968 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0801 02:13:37.097307 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:13:38.020066 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:13:38.860651 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:13:39.538608 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:13:40.294615 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:13:40.976842 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:13:41.742346 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n",
      "I0801 02:13:42.418427 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:13:43.171011 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:13:43.854803 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
      "I0801 02:13:45.285345 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0801 02:13:46.805942 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0801 02:13:48.365183 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:13:49.209702 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:13:50.025828 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:13:50.789445 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:13:51.593183 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:13:52.449072 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:13:53.141094 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
      "I0801 02:13:53.984822 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:13:54.749080 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:13:55.462852 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
      "I0801 02:13:56.282218 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:13:57.233350 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:13:57.970997 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
      "I0801 02:13:59.001968 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/key/bias/adam_m with shape [768]\n",
      "I0801 02:13:59.805926 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/key/bias/adam_v with shape [768]\n",
      "I0801 02:14:00.529660 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
      "I0801 02:14:01.436505 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:14:02.333997 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:14:03.127079 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
      "I0801 02:14:03.972823 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/query/bias/adam_m with shape [768]\n",
      "I0801 02:14:04.763419 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/query/bias/adam_v with shape [768]\n",
      "I0801 02:14:05.439158 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
      "I0801 02:14:06.298141 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:14:07.124912 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:14:07.985972 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
      "I0801 02:14:08.872240 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/value/bias/adam_m with shape [768]\n",
      "I0801 02:14:09.602194 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/value/bias/adam_v with shape [768]\n",
      "I0801 02:14:10.485298 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
      "I0801 02:14:11.342661 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:14:12.129908 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:14:13.034727 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
      "I0801 02:14:13.795394 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0801 02:14:14.471097 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0801 02:14:15.227350 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0801 02:14:16.578570 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0801 02:14:18.097820 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0801 02:14:19.578012 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:14:20.461122 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:14:21.264839 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:14:21.989932 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:14:22.753415 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:14:23.636664 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:14:24.274890 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n",
      "I0801 02:14:25.070401 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:14:25.710919 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:14:26.467551 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
      "I0801 02:14:27.779276 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0801 02:14:29.337867 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0801 02:14:30.977442 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:14:31.821393 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:14:32.625330 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:14:33.352088 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:14:34.156945 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:14:35.005066 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:14:35.693119 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
      "I0801 02:14:36.536870 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:14:37.301485 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:14:38.029223 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
      "I0801 02:14:38.925837 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:14:39.912520 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:14:40.690857 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
      "I0801 02:14:41.534389 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/key/bias/adam_m with shape [768]\n",
      "I0801 02:14:42.342058 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/key/bias/adam_v with shape [768]\n",
      "I0801 02:14:43.066096 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
      "I0801 02:14:43.976813 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:14:44.878690 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:14:45.669692 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
      "I0801 02:14:46.481592 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/query/bias/adam_m with shape [768]\n",
      "I0801 02:14:47.324888 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/query/bias/adam_v with shape [768]\n",
      "I0801 02:14:48.052850 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
      "I0801 02:14:48.910183 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:14:49.817766 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:14:50.716396 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
      "I0801 02:14:51.525207 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/value/bias/adam_m with shape [768]\n",
      "I0801 02:14:52.330756 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/value/bias/adam_v with shape [768]\n",
      "I0801 02:14:53.094240 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
      "I0801 02:14:54.000976 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:14:54.898274 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:14:55.689074 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
      "I0801 02:14:56.497459 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0801 02:14:57.340925 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0801 02:14:58.069062 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0801 02:14:59.548793 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0801 02:15:01.080523 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0801 02:15:02.558363 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:15:03.402216 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:15:04.206119 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:15:04.930054 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:15:05.694310 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:15:06.577787 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:15:07.269770 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n",
      "I0801 02:15:08.113248 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:15:08.917119 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:15:09.652944 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
      "I0801 02:15:11.141238 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0801 02:15:12.621201 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0801 02:15:14.130702 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:15:14.974236 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:15:15.782040 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:15:16.470169 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:15:17.274168 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:15:18.117895 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:15:18.845180 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
      "I0801 02:15:19.649946 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:15:20.493238 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:15:21.181498 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
      "I0801 02:15:22.078617 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:15:22.993889 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:15:23.775143 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
      "I0801 02:15:24.587230 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/key/bias/adam_m with shape [768]\n",
      "I0801 02:15:25.430287 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/key/bias/adam_v with shape [768]\n",
      "I0801 02:15:26.158643 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
      "I0801 02:15:27.065003 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:15:27.966415 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:15:28.717821 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
      "I0801 02:15:29.601429 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/query/bias/adam_m with shape [768]\n",
      "I0801 02:15:30.408953 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/query/bias/adam_v with shape [768]\n",
      "I0801 02:15:31.132849 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
      "I0801 02:15:32.134385 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:15:33.045898 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:15:33.822946 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
      "I0801 02:15:34.666864 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/value/bias/adam_m with shape [768]\n",
      "I0801 02:15:35.474924 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/value/bias/adam_v with shape [768]\n",
      "I0801 02:15:36.206898 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
      "I0801 02:15:37.117278 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:15:38.014999 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:15:38.925571 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
      "I0801 02:15:39.774163 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0801 02:15:40.581578 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0801 02:15:41.309725 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0801 02:15:42.793275 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0801 02:15:44.324693 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0801 02:15:45.799094 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:15:46.646926 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:15:47.465121 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:15:48.194525 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:15:49.118304 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:15:49.965843 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:15:50.653648 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n",
      "I0801 02:15:51.497521 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:15:52.309629 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:15:53.037124 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
      "I0801 02:15:54.517430 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0801 02:15:56.044893 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0801 02:15:57.514720 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:15:58.360366 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:15:59.286362 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:16:00.014221 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:16:00.817920 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:16:01.661872 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:16:02.349636 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
      "I0801 02:16:03.193106 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:16:04.009297 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:16:04.737216 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
      "I0801 02:16:05.594915 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:16:06.553608 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:16:07.294676 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
      "I0801 02:16:08.142494 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/key/bias/adam_m with shape [768]\n",
      "I0801 02:16:08.984474 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/key/bias/adam_v with shape [768]\n",
      "I0801 02:16:09.790203 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
      "I0801 02:16:10.697089 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:16:11.598569 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:16:12.389569 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
      "I0801 02:16:13.241336 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/query/bias/adam_m with shape [768]\n",
      "I0801 02:16:14.045092 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/query/bias/adam_v with shape [768]\n",
      "I0801 02:16:14.769082 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
      "I0801 02:16:15.626747 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:16:16.542333 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:16:17.368419 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
      "I0801 02:16:18.162566 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/value/bias/adam_m with shape [768]\n",
      "I0801 02:16:18.798525 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/value/bias/adam_v with shape [768]\n",
      "I0801 02:16:19.710583 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
      "I0801 02:16:20.565923 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:16:21.392750 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:16:22.258567 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
      "I0801 02:16:23.102202 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0801 02:16:23.785688 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0801 02:16:24.590346 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0801 02:16:26.108969 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0801 02:16:27.629096 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0801 02:16:29.098960 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:16:30.058540 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:16:30.866118 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:16:31.594119 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:16:32.397854 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:16:33.241554 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:16:33.929456 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n",
      "I0801 02:16:34.773509 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:16:35.585151 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:16:36.309376 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
      "I0801 02:16:37.789016 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0801 02:16:39.259239 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0801 02:16:40.810691 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:16:41.658314 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:16:42.465992 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:16:43.189778 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:16:43.993488 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:16:44.837450 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:16:45.533252 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
      "I0801 02:16:46.377472 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:16:47.193005 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:16:47.916896 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
      "I0801 02:16:48.778339 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:16:49.725329 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:16:50.504836 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
      "I0801 02:16:51.350525 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/key/bias/adam_m with shape [768]\n",
      "I0801 02:16:52.166667 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/key/bias/adam_v with shape [768]\n",
      "I0801 02:16:52.894261 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
      "I0801 02:16:53.801190 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:16:54.702573 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:16:55.501254 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
      "I0801 02:16:56.345288 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/query/bias/adam_m with shape [768]\n",
      "I0801 02:16:57.149482 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/query/bias/adam_v with shape [768]\n",
      "I0801 02:16:57.873296 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
      "I0801 02:16:58.691527 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:16:59.677623 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:17:00.414599 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
      "I0801 02:17:01.346498 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/value/bias/adam_m with shape [768]\n",
      "I0801 02:17:02.110819 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/value/bias/adam_v with shape [768]\n",
      "I0801 02:17:02.839208 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
      "I0801 02:17:03.773279 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:17:04.671061 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:17:05.461545 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
      "I0801 02:17:06.306065 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0801 02:17:07.113345 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0801 02:17:07.841406 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0801 02:17:09.324692 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0801 02:17:10.805077 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0801 02:17:12.356570 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
      "I0801 02:17:13.242868 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:17:14.051311 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:17:14.782042 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
      "I0801 02:17:15.585685 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:17:16.390312 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:17:17.117374 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n",
      "I0801 02:17:17.960908 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/dense/bias/adam_m with shape [768]\n",
      "I0801 02:17:18.765065 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/dense/bias/adam_v with shape [768]\n",
      "I0801 02:17:19.489020 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
      "I0801 02:17:20.968797 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0801 02:17:22.519430 140230207002432 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0801 02:17:24.030455 140230207002432 modeling_bert.py:90] Loading TF weight bert/pooler/dense/bias with shape [768]\n",
      "I0801 02:17:24.886343 140230207002432 modeling_bert.py:90] Loading TF weight bert/pooler/dense/bias/adam_m with shape [768]\n",
      "I0801 02:17:25.658821 140230207002432 modeling_bert.py:90] Loading TF weight bert/pooler/dense/bias/adam_v with shape [768]\n",
      "I0801 02:17:26.382775 140230207002432 modeling_bert.py:90] Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
      "I0801 02:17:27.329629 140230207002432 modeling_bert.py:90] Loading TF weight bert/pooler/dense/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:17:28.186740 140230207002432 modeling_bert.py:90] Loading TF weight bert/pooler/dense/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:17:29.017202 140230207002432 modeling_bert.py:90] Loading TF weight cls/predictions/output_bias with shape [30522]\n",
      "I0801 02:17:29.861208 140230207002432 modeling_bert.py:90] Loading TF weight cls/predictions/output_bias/adam_m with shape [30522]\n",
      "I0801 02:17:30.625899 140230207002432 modeling_bert.py:90] Loading TF weight cls/predictions/output_bias/adam_v with shape [30522]\n",
      "I0801 02:17:31.357786 140230207002432 modeling_bert.py:90] Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [768]\n",
      "I0801 02:17:32.161943 140230207002432 modeling_bert.py:90] Loading TF weight cls/predictions/transform/LayerNorm/beta/adam_m with shape [768]\n",
      "I0801 02:17:33.045518 140230207002432 modeling_bert.py:90] Loading TF weight cls/predictions/transform/LayerNorm/beta/adam_v with shape [768]\n",
      "I0801 02:17:33.729772 140230207002432 modeling_bert.py:90] Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [768]\n",
      "I0801 02:17:34.620824 140230207002432 modeling_bert.py:90] Loading TF weight cls/predictions/transform/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0801 02:17:35.424670 140230207002432 modeling_bert.py:90] Loading TF weight cls/predictions/transform/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0801 02:17:36.148874 140230207002432 modeling_bert.py:90] Loading TF weight cls/predictions/transform/dense/bias with shape [768]\n",
      "I0801 02:17:36.952743 140230207002432 modeling_bert.py:90] Loading TF weight cls/predictions/transform/dense/bias/adam_m with shape [768]\n",
      "I0801 02:17:37.796959 140230207002432 modeling_bert.py:90] Loading TF weight cls/predictions/transform/dense/bias/adam_v with shape [768]\n",
      "I0801 02:17:38.391671 140230207002432 modeling_bert.py:90] Loading TF weight cls/predictions/transform/dense/kernel with shape [768, 768]\n",
      "I0801 02:17:39.286432 140230207002432 modeling_bert.py:90] Loading TF weight cls/predictions/transform/dense/kernel/adam_m with shape [768, 768]\n",
      "I0801 02:17:40.233098 140230207002432 modeling_bert.py:90] Loading TF weight cls/predictions/transform/dense/kernel/adam_v with shape [768, 768]\n",
      "I0801 02:17:40.899156 140230207002432 modeling_bert.py:90] Loading TF weight cls/seq_relationship/output_bias with shape [2]\n",
      "I0801 02:17:41.541139 140230207002432 modeling_bert.py:90] Loading TF weight cls/seq_relationship/output_bias/adam_m with shape [2]\n",
      "I0801 02:17:42.097058 140230207002432 modeling_bert.py:90] Loading TF weight cls/seq_relationship/output_bias/adam_v with shape [2]\n",
      "I0801 02:17:42.849843 140230207002432 modeling_bert.py:90] Loading TF weight cls/seq_relationship/output_weights with shape [2, 768]\n",
      "I0801 02:17:43.408700 140230207002432 modeling_bert.py:90] Loading TF weight cls/seq_relationship/output_weights/adam_m with shape [2, 768]\n",
      "I0801 02:17:44.042909 140230207002432 modeling_bert.py:90] Loading TF weight cls/seq_relationship/output_weights/adam_v with shape [2, 768]\n",
      "I0801 02:17:44.600381 140230207002432 modeling_bert.py:90] Loading TF weight global_step with shape []\n",
      "I0801 02:17:45.240065 140230207002432 modeling_bert.py:90] Loading TF weight good_steps with shape []\n",
      "I0801 02:17:45.795120 140230207002432 modeling_bert.py:90] Loading TF weight loss_scale with shape []\n",
      "I0801 02:17:46.451653 140230207002432 modeling_bert.py:100] Skipping bad_steps\n",
      "FinBERT Converting: ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.452191 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.452414 140230207002432 modeling_bert.py:100] Skipping bert/embeddings/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.452498 140230207002432 modeling_bert.py:100] Skipping bert/embeddings/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.452605 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.452694 140230207002432 modeling_bert.py:100] Skipping bert/embeddings/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.452760 140230207002432 modeling_bert.py:100] Skipping bert/embeddings/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'embeddings', 'position_embeddings']\n",
      "I0801 02:17:46.452862 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
      "I0801 02:17:46.453092 140230207002432 modeling_bert.py:100] Skipping bert/embeddings/position_embeddings/adam_m\n",
      "I0801 02:17:46.453167 140230207002432 modeling_bert.py:100] Skipping bert/embeddings/position_embeddings/adam_v\n",
      "FinBERT Converting: ['bert', 'embeddings', 'token_type_embeddings']\n",
      "I0801 02:17:46.453269 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
      "I0801 02:17:46.453349 140230207002432 modeling_bert.py:100] Skipping bert/embeddings/token_type_embeddings/adam_m\n",
      "I0801 02:17:46.453413 140230207002432 modeling_bert.py:100] Skipping bert/embeddings/token_type_embeddings/adam_v\n",
      "FinBERT Converting: ['bert', 'embeddings', 'word_embeddings']\n",
      "I0801 02:17:46.453508 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
      "I0801 02:17:46.460780 140230207002432 modeling_bert.py:100] Skipping bert/embeddings/word_embeddings/adam_m\n",
      "I0801 02:17:46.460864 140230207002432 modeling_bert.py:100] Skipping bert/embeddings/word_embeddings/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.461141 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.461242 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.461312 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.461434 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.461514 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.461583 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.461704 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.461809 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/output/dense/bias/adam_m\n",
      "I0801 02:17:46.461875 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.462011 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.462280 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.462354 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.462479 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.462559 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/key/bias/adam_m\n",
      "I0801 02:17:46.462625 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.462746 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.463018 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/key/kernel/adam_m\n",
      "I0801 02:17:46.463090 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.463209 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.463291 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/query/bias/adam_m\n",
      "I0801 02:17:46.463353 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.463473 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.463735 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/query/kernel/adam_m\n",
      "I0801 02:17:46.463805 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.463924 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.464003 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/value/bias/adam_m\n",
      "I0801 02:17:46.464066 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.464186 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.464447 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/value/kernel/adam_m\n",
      "I0801 02:17:46.464521 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.464639 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.464719 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/intermediate/dense/bias/adam_m\n",
      "I0801 02:17:46.464783 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.464897 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.465668 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/intermediate/dense/kernel/adam_m\n",
      "I0801 02:17:46.465760 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.465887 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.465971 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.466036 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.466149 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.466226 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.466290 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.466400 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.466477 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/output/dense/bias/adam_m\n",
      "I0801 02:17:46.466541 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.466658 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.467420 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.467494 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_0/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.467620 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.467701 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.467767 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.467883 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.467961 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.468025 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.468140 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.468218 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/output/dense/bias/adam_m\n",
      "I0801 02:17:46.468282 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.468403 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.468667 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.468731 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.468852 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.468937 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/key/bias/adam_m\n",
      "I0801 02:17:46.469005 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.469127 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.469399 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/key/kernel/adam_m\n",
      "I0801 02:17:46.469472 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.469592 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.469674 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/query/bias/adam_m\n",
      "I0801 02:17:46.469753 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.469879 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.470144 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/query/kernel/adam_m\n",
      "I0801 02:17:46.470217 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.470336 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.470415 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/value/bias/adam_m\n",
      "I0801 02:17:46.470479 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.470598 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.470865 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/value/kernel/adam_m\n",
      "I0801 02:17:46.470937 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.471046 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.471126 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/intermediate/dense/bias/adam_m\n",
      "I0801 02:17:46.471190 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.471304 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.472068 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/intermediate/dense/kernel/adam_m\n",
      "I0801 02:17:46.472144 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.472264 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.472344 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.472410 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.472521 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.472599 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.472662 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.472767 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.472843 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/output/dense/bias/adam_m\n",
      "I0801 02:17:46.472906 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.473021 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.473812 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.473889 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_1/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.474011 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.474093 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.474159 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.474275 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.474354 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.474419 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.474534 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.474612 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/output/dense/bias/adam_m\n",
      "I0801 02:17:46.474675 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.474796 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.475030 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.475102 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.475224 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.475307 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/key/bias/adam_m\n",
      "I0801 02:17:46.475370 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.475484 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.475716 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/key/kernel/adam_m\n",
      "I0801 02:17:46.475787 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.475907 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.475987 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/query/bias/adam_m\n",
      "I0801 02:17:46.476050 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.476169 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.476399 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/query/kernel/adam_m\n",
      "I0801 02:17:46.476471 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.476589 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.476670 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/value/bias/adam_m\n",
      "I0801 02:17:46.476733 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.476851 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.477081 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/value/kernel/adam_m\n",
      "I0801 02:17:46.477152 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.477269 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.477347 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/intermediate/dense/bias/adam_m\n",
      "I0801 02:17:46.477410 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.477524 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.478178 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/intermediate/dense/kernel/adam_m\n",
      "I0801 02:17:46.478255 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.478375 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.478454 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.478518 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.478628 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.478704 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.478767 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.478875 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.478954 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/output/dense/bias/adam_m\n",
      "I0801 02:17:46.479018 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.479133 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.479773 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.479845 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_10/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.479969 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.480049 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.480113 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.480229 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.480306 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.480370 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.480479 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.480556 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/output/dense/bias/adam_m\n",
      "I0801 02:17:46.480621 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.480740 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.480977 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.481047 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.481168 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.481245 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/key/bias/adam_m\n",
      "I0801 02:17:46.481309 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.481427 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.481663 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/key/kernel/adam_m\n",
      "I0801 02:17:46.481777 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.481905 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.481987 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/query/bias/adam_m\n",
      "I0801 02:17:46.482052 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.482163 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.482398 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/query/kernel/adam_m\n",
      "I0801 02:17:46.482471 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.482591 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.482671 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/value/bias/adam_m\n",
      "I0801 02:17:46.482734 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.482853 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.483078 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/value/kernel/adam_m\n",
      "I0801 02:17:46.483149 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.483265 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.483343 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/intermediate/dense/bias/adam_m\n",
      "I0801 02:17:46.483407 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.483521 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.484170 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/intermediate/dense/kernel/adam_m\n",
      "I0801 02:17:46.484243 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.484362 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.484441 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.484505 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.484615 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.484691 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.484755 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.484861 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.484937 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/output/dense/bias/adam_m\n",
      "I0801 02:17:46.485000 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.485114 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.485799 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.485875 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_11/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.486001 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.486083 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.486149 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.486265 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.486342 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.486406 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.486520 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.486596 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/output/dense/bias/adam_m\n",
      "I0801 02:17:46.486658 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.486800 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.487042 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.487112 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.487230 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.487307 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/key/bias/adam_m\n",
      "I0801 02:17:46.487371 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.487488 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.487749 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/key/kernel/adam_m\n",
      "I0801 02:17:46.487820 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.487936 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.488015 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/query/bias/adam_m\n",
      "I0801 02:17:46.488078 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.488194 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.488459 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/query/kernel/adam_m\n",
      "I0801 02:17:46.488529 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.488644 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.488722 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/value/bias/adam_m\n",
      "I0801 02:17:46.488783 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.488898 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.489131 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/value/kernel/adam_m\n",
      "I0801 02:17:46.489204 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.489321 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.489400 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/intermediate/dense/bias/adam_m\n",
      "I0801 02:17:46.489463 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.489580 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.490233 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/intermediate/dense/kernel/adam_m\n",
      "I0801 02:17:46.490308 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.490421 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.490502 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.490566 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.490677 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.490754 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.490810 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.490918 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.490986 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/output/dense/bias/adam_m\n",
      "I0801 02:17:46.491049 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.491163 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.491795 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.491867 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_2/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.491992 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.492071 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.492137 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.492250 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.492327 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.492391 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.492506 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.492583 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/output/dense/bias/adam_m\n",
      "I0801 02:17:46.492647 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.492760 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.492990 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.493060 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.493181 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.493260 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/key/bias/adam_m\n",
      "I0801 02:17:46.493326 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.493436 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.493666 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/key/kernel/adam_m\n",
      "I0801 02:17:46.493762 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.493889 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.493970 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/query/bias/adam_m\n",
      "I0801 02:17:46.494035 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.494157 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.494390 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/query/kernel/adam_m\n",
      "I0801 02:17:46.494463 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.494580 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.494661 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/value/bias/adam_m\n",
      "I0801 02:17:46.494724 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.494842 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.495071 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/value/kernel/adam_m\n",
      "I0801 02:17:46.495142 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.495259 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.495337 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/intermediate/dense/bias/adam_m\n",
      "I0801 02:17:46.495400 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.495516 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.496157 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/intermediate/dense/kernel/adam_m\n",
      "I0801 02:17:46.496231 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.496348 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.496429 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.496493 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.496603 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.496680 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.496742 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.496844 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.496919 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/output/dense/bias/adam_m\n",
      "I0801 02:17:46.496981 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.497095 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.497730 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.497853 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_3/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.497979 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.498063 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.498129 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.498243 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.498321 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.498384 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.498498 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.498575 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/output/dense/bias/adam_m\n",
      "I0801 02:17:46.498638 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.498756 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.499000 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.499070 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.499188 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.499266 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/key/bias/adam_m\n",
      "I0801 02:17:46.499328 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.499443 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.499674 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/key/kernel/adam_m\n",
      "I0801 02:17:46.499744 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.499860 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.499938 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/query/bias/adam_m\n",
      "I0801 02:17:46.500000 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.500116 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.500351 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/query/kernel/adam_m\n",
      "I0801 02:17:46.500422 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.500540 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.500621 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/value/bias/adam_m\n",
      "I0801 02:17:46.500684 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.500802 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.501045 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/value/kernel/adam_m\n",
      "I0801 02:17:46.501116 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.501238 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.501317 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/intermediate/dense/bias/adam_m\n",
      "I0801 02:17:46.501380 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.501498 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.502175 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/intermediate/dense/kernel/adam_m\n",
      "I0801 02:17:46.502251 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.502372 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.502454 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.502517 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.502631 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.502707 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.502770 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.502889 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.502965 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/output/dense/bias/adam_m\n",
      "I0801 02:17:46.503027 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.503144 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.503795 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.503869 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_4/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.503988 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.504068 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.504133 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.504249 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.504326 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.504391 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.504501 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.504579 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/output/dense/bias/adam_m\n",
      "I0801 02:17:46.504644 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.504764 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.504998 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.505062 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.505179 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.505259 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/key/bias/adam_m\n",
      "I0801 02:17:46.505324 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.505444 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.505667 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/key/kernel/adam_m\n",
      "I0801 02:17:46.505757 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.505883 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.505965 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/query/bias/adam_m\n",
      "I0801 02:17:46.506029 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.506149 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.506378 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/query/kernel/adam_m\n",
      "I0801 02:17:46.506451 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.506563 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.506642 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/value/bias/adam_m\n",
      "I0801 02:17:46.506706 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.506825 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.507057 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/value/kernel/adam_m\n",
      "I0801 02:17:46.507129 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.507246 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.507324 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/intermediate/dense/bias/adam_m\n",
      "I0801 02:17:46.507387 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.507503 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.508161 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/intermediate/dense/kernel/adam_m\n",
      "I0801 02:17:46.508234 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.508352 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.508432 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.508494 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.508605 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.508679 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.508741 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.508851 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.508927 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/output/dense/bias/adam_m\n",
      "I0801 02:17:46.508996 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.509110 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.509796 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.509871 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_5/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.509997 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.510081 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.510146 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.510261 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.510339 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.510401 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.510517 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.510594 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/output/dense/bias/adam_m\n",
      "I0801 02:17:46.510657 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.510771 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.511008 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.511079 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.511200 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.511279 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/key/bias/adam_m\n",
      "I0801 02:17:46.511342 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.511462 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.511703 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/key/kernel/adam_m\n",
      "I0801 02:17:46.511775 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.511893 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.511967 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/query/bias/adam_m\n",
      "I0801 02:17:46.512030 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.512150 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.512388 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/query/kernel/adam_m\n",
      "I0801 02:17:46.512459 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.512578 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.512657 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/value/bias/adam_m\n",
      "I0801 02:17:46.512721 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.512833 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.513067 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/value/kernel/adam_m\n",
      "I0801 02:17:46.513139 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.513258 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.513338 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/intermediate/dense/bias/adam_m\n",
      "I0801 02:17:46.513402 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.513511 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.514215 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/intermediate/dense/kernel/adam_m\n",
      "I0801 02:17:46.514292 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.514414 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.514496 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.514559 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.514671 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.514750 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.514812 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.514923 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.515000 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/output/dense/bias/adam_m\n",
      "I0801 02:17:46.515062 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.515178 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.515853 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.515926 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_6/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.516051 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.516132 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.516197 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.516314 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.516391 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.516449 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.516563 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.516641 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/output/dense/bias/adam_m\n",
      "I0801 02:17:46.516706 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.516824 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.517066 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.517137 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.517259 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.517339 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/key/bias/adam_m\n",
      "I0801 02:17:46.517405 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.517518 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.517806 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/key/kernel/adam_m\n",
      "I0801 02:17:46.517881 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.518004 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.518086 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/query/bias/adam_m\n",
      "I0801 02:17:46.518151 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.518271 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.518507 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/query/kernel/adam_m\n",
      "I0801 02:17:46.518578 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.518697 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.518776 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/value/bias/adam_m\n",
      "I0801 02:17:46.518840 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.518964 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.519211 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/value/kernel/adam_m\n",
      "I0801 02:17:46.519283 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.519402 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.519483 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/intermediate/dense/bias/adam_m\n",
      "I0801 02:17:46.519539 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.519649 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.520308 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/intermediate/dense/kernel/adam_m\n",
      "I0801 02:17:46.520382 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.520499 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.520579 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.520644 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.520753 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.520830 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.520893 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.521002 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.521078 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/output/dense/bias/adam_m\n",
      "I0801 02:17:46.521141 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.521254 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.521940 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.522016 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_7/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.522144 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.522225 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.522291 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.522408 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.522486 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.522550 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.522664 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.522742 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/output/dense/bias/adam_m\n",
      "I0801 02:17:46.522805 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.522925 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.523161 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.523232 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.523354 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.523432 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/key/bias/adam_m\n",
      "I0801 02:17:46.523489 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.523610 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.523845 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/key/kernel/adam_m\n",
      "I0801 02:17:46.523917 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.524029 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.524110 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/query/bias/adam_m\n",
      "I0801 02:17:46.524173 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.524293 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.524530 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/query/kernel/adam_m\n",
      "I0801 02:17:46.524603 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.524723 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.524803 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/value/bias/adam_m\n",
      "I0801 02:17:46.524867 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.524986 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.525226 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/value/kernel/adam_m\n",
      "I0801 02:17:46.525297 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.525418 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.525498 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/intermediate/dense/bias/adam_m\n",
      "I0801 02:17:46.525563 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.525672 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.526364 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/intermediate/dense/kernel/adam_m\n",
      "I0801 02:17:46.526439 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.526559 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.526640 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.526704 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.526815 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.526892 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.526957 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.527063 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.527140 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/output/dense/bias/adam_m\n",
      "I0801 02:17:46.527196 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.527304 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.527965 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.528038 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_8/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.528164 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.528244 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.528310 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.528418 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.528496 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.528559 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.528675 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.528752 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/output/dense/bias/adam_m\n",
      "I0801 02:17:46.528818 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.528933 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.529179 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.529252 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.529373 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
      "I0801 02:17:46.529453 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/key/bias/adam_m\n",
      "I0801 02:17:46.529518 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.529636 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
      "I0801 02:17:46.529897 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/key/kernel/adam_m\n",
      "I0801 02:17:46.529965 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.530085 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
      "I0801 02:17:46.530165 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/query/bias/adam_m\n",
      "I0801 02:17:46.530229 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.530349 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
      "I0801 02:17:46.530586 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/query/kernel/adam_m\n",
      "I0801 02:17:46.530658 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.530776 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
      "I0801 02:17:46.530856 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/value/bias/adam_m\n",
      "I0801 02:17:46.530921 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.531040 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
      "I0801 02:17:46.531279 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/value/kernel/adam_m\n",
      "I0801 02:17:46.531351 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.531467 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
      "I0801 02:17:46.531547 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/intermediate/dense/bias/adam_m\n",
      "I0801 02:17:46.531610 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.531726 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
      "I0801 02:17:46.532405 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/intermediate/dense/kernel/adam_m\n",
      "I0801 02:17:46.532479 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.532599 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.532680 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/output/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.532744 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.532848 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.532925 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/output/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.532988 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.533099 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
      "I0801 02:17:46.533174 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/output/dense/bias/adam_m\n",
      "I0801 02:17:46.533237 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.533346 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
      "I0801 02:17:46.534046 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/output/dense/kernel/adam_m\n",
      "I0801 02:17:46.534123 140230207002432 modeling_bert.py:100] Skipping bert/encoder/layer_9/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'pooler', 'dense', 'bias']\n",
      "I0801 02:17:46.534229 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
      "I0801 02:17:46.534307 140230207002432 modeling_bert.py:100] Skipping bert/pooler/dense/bias/adam_m\n",
      "I0801 02:17:46.534369 140230207002432 modeling_bert.py:100] Skipping bert/pooler/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'pooler', 'dense', 'kernel']\n",
      "I0801 02:17:46.534463 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
      "I0801 02:17:46.534699 140230207002432 modeling_bert.py:100] Skipping bert/pooler/dense/kernel/adam_m\n",
      "I0801 02:17:46.534769 140230207002432 modeling_bert.py:100] Skipping bert/pooler/dense/kernel/adam_v\n",
      "FinBERT Converting: ['cls', 'predictions', 'output_bias']\n",
      "I0801 02:17:46.534860 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['cls', 'predictions', 'output_bias']\n",
      "I0801 02:17:46.534935 140230207002432 modeling_bert.py:100] Skipping cls/predictions/output_bias/adam_m\n",
      "I0801 02:17:46.534996 140230207002432 modeling_bert.py:100] Skipping cls/predictions/output_bias/adam_v\n",
      "FinBERT Converting: ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.535097 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\n",
      "I0801 02:17:46.535170 140230207002432 modeling_bert.py:100] Skipping cls/predictions/transform/LayerNorm/beta/adam_m\n",
      "I0801 02:17:46.535231 140230207002432 modeling_bert.py:100] Skipping cls/predictions/transform/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.535326 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\n",
      "I0801 02:17:46.535398 140230207002432 modeling_bert.py:100] Skipping cls/predictions/transform/LayerNorm/gamma/adam_m\n",
      "I0801 02:17:46.535459 140230207002432 modeling_bert.py:100] Skipping cls/predictions/transform/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['cls', 'predictions', 'transform', 'dense', 'bias']\n",
      "I0801 02:17:46.535554 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\n",
      "I0801 02:17:46.535627 140230207002432 modeling_bert.py:100] Skipping cls/predictions/transform/dense/bias/adam_m\n",
      "I0801 02:17:46.535689 140230207002432 modeling_bert.py:100] Skipping cls/predictions/transform/dense/bias/adam_v\n",
      "FinBERT Converting: ['cls', 'predictions', 'transform', 'dense', 'kernel']\n",
      "I0801 02:17:46.535789 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\n",
      "I0801 02:17:46.536036 140230207002432 modeling_bert.py:100] Skipping cls/predictions/transform/dense/kernel/adam_m\n",
      "I0801 02:17:46.536108 140230207002432 modeling_bert.py:100] Skipping cls/predictions/transform/dense/kernel/adam_v\n",
      "FinBERT Converting: ['cls', 'seq_relationship', 'output_bias']\n",
      "I0801 02:17:46.536207 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\n",
      "I0801 02:17:46.536285 140230207002432 modeling_bert.py:100] Skipping cls/seq_relationship/output_bias/adam_m\n",
      "I0801 02:17:46.536348 140230207002432 modeling_bert.py:100] Skipping cls/seq_relationship/output_bias/adam_v\n",
      "FinBERT Converting: ['cls', 'seq_relationship', 'output_weights']\n",
      "I0801 02:17:46.536440 140230207002432 modeling_bert.py:137] Initialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\n",
      "I0801 02:17:46.536514 140230207002432 modeling_bert.py:100] Skipping cls/seq_relationship/output_weights/adam_m\n",
      "I0801 02:17:46.536575 140230207002432 modeling_bert.py:100] Skipping cls/seq_relationship/output_weights/adam_v\n",
      "I0801 02:17:46.536631 140230207002432 modeling_bert.py:100] Skipping global_step\n",
      "I0801 02:17:46.536686 140230207002432 modeling_bert.py:100] Skipping good_steps\n",
      "I0801 02:17:46.536741 140230207002432 modeling_bert.py:100] Skipping loss_scale\n",
      "Save PyTorch model to FinBERT-Combo_128MSL-250K.bin\n"
     ]
    }
   ],
   "source": [
    "convert('gold/FinBERT-Combo_128MSL-250K/model.ckpt-250000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building PyTorch model from configuration: {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0731 21:52:34.027374 139833534199616 modeling_bert.py:84] Converting TensorFlow checkpoint from /root/w266-final/pred/gold/FinBERT-Prime_128MSL-250K/model.ckpt-250000\n",
      "I0731 21:52:34.585040 139833534199616 modeling_bert.py:90] Loading TF weight bad_steps with shape []\n",
      "I0731 21:52:35.230917 139833534199616 modeling_bert.py:90] Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n",
      "I0731 21:52:36.114815 139833534199616 modeling_bert.py:90] Loading TF weight bert/embeddings/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 21:52:36.878849 139833534199616 modeling_bert.py:90] Loading TF weight bert/embeddings/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 21:52:37.604034 139833534199616 modeling_bert.py:90] Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n",
      "I0731 21:52:38.445992 139833534199616 modeling_bert.py:90] Loading TF weight bert/embeddings/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 21:52:39.250364 139833534199616 modeling_bert.py:90] Loading TF weight bert/embeddings/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 21:52:39.981525 139833534199616 modeling_bert.py:90] Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n",
      "I0731 21:52:40.825195 139833534199616 modeling_bert.py:90] Loading TF weight bert/embeddings/position_embeddings/adam_m with shape [512, 768]\n",
      "I0731 21:52:41.589814 139833534199616 modeling_bert.py:90] Loading TF weight bert/embeddings/position_embeddings/adam_v with shape [512, 768]\n",
      "I0731 21:52:42.393607 139833534199616 modeling_bert.py:90] Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n",
      "I0731 21:52:43.197704 139833534199616 modeling_bert.py:90] Loading TF weight bert/embeddings/token_type_embeddings/adam_m with shape [2, 768]\n",
      "I0731 21:52:44.041562 139833534199616 modeling_bert.py:90] Loading TF weight bert/embeddings/token_type_embeddings/adam_v with shape [2, 768]\n",
      "I0731 21:52:44.768793 139833534199616 modeling_bert.py:90] Loading TF weight bert/embeddings/word_embeddings with shape [30522, 768]\n",
      "I0731 21:52:52.996933 139833534199616 modeling_bert.py:90] Loading TF weight bert/embeddings/word_embeddings/adam_m with shape [30522, 768]\n",
      "I0731 21:53:01.186015 139833534199616 modeling_bert.py:90] Loading TF weight bert/embeddings/word_embeddings/adam_v with shape [30522, 768]\n",
      "I0731 21:53:09.425127 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
      "I0731 21:53:10.245214 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 21:53:11.088541 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 21:53:11.723285 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0731 21:53:12.514763 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 21:53:13.387312 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 21:53:14.063153 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
      "I0731 21:53:14.815260 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0731 21:53:15.490425 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0731 21:53:16.282738 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
      "I0731 21:53:17.021876 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:53:17.968829 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:53:18.790580 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
      "I0731 21:53:19.554038 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/key/bias/adam_m with shape [768]\n",
      "I0731 21:53:20.322803 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/key/bias/adam_v with shape [768]\n",
      "I0731 21:53:21.166186 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
      "I0731 21:53:21.992517 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:53:22.930043 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:53:23.952955 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
      "I0731 21:53:24.676907 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/query/bias/adam_m with shape [768]\n",
      "I0731 21:53:25.441531 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/query/bias/adam_v with shape [768]\n",
      "I0731 21:53:26.324990 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
      "I0731 21:53:27.066236 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:53:28.013263 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:53:28.832634 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
      "I0731 21:53:29.604287 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/value/bias/adam_m with shape [768]\n",
      "I0731 21:53:30.410243 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/value/bias/adam_v with shape [768]\n",
      "I0731 21:53:31.214273 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
      "I0731 21:53:32.044730 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:53:32.962450 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:53:33.909518 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
      "I0731 21:53:34.637221 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0731 21:53:35.440944 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0731 21:53:36.285131 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0731 21:53:37.654204 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0731 21:53:39.141695 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0731 21:53:40.700990 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
      "I0731 21:53:41.548820 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 21:53:42.317373 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 21:53:43.088906 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
      "I0731 21:53:43.892797 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 21:53:44.857142 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 21:53:45.544716 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n",
      "I0731 21:53:46.339125 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/dense/bias/adam_m with shape [768]\n",
      "I0731 21:53:46.979099 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/dense/bias/adam_v with shape [768]\n",
      "I0731 21:53:47.771022 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
      "I0731 21:53:49.264832 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0731 21:53:50.750248 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_0/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0731 21:53:52.273926 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
      "I0731 21:53:53.117933 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 21:53:53.882098 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 21:53:54.613888 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0731 21:53:55.533411 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 21:53:56.337722 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 21:53:57.061257 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
      "I0731 21:53:57.905177 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0731 21:53:58.669536 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0731 21:53:59.394301 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
      "I0731 21:54:00.296124 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:54:01.205959 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:54:02.035217 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
      "I0731 21:54:02.890327 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/key/bias/adam_m with shape [768]\n",
      "I0731 21:54:03.694379 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/key/bias/adam_v with shape [768]\n",
      "I0731 21:54:04.422097 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
      "I0731 21:54:05.329169 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:54:06.246724 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:54:06.998163 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
      "I0731 21:54:07.881231 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/query/bias/adam_m with shape [768]\n",
      "I0731 21:54:08.689105 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/query/bias/adam_v with shape [768]\n",
      "I0731 21:54:09.377593 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
      "I0731 21:54:10.235235 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:54:11.221943 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:54:12.012877 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
      "I0731 21:54:12.817430 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/value/bias/adam_m with shape [768]\n",
      "I0731 21:54:13.621402 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/value/bias/adam_v with shape [768]\n",
      "I0731 21:54:14.384746 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
      "I0731 21:54:15.250203 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:54:16.285516 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:54:17.058830 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
      "I0731 21:54:17.902584 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0731 21:54:18.706343 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0731 21:54:19.430253 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0731 21:54:20.917859 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0731 21:54:22.477133 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0731 21:54:24.001016 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
      "I0731 21:54:24.844755 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 21:54:25.648695 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 21:54:26.442605 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
      "I0731 21:54:27.198650 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 21:54:27.875421 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 21:54:28.626670 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n",
      "I0731 21:54:29.302884 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/dense/bias/adam_m with shape [768]\n",
      "I0731 21:54:30.106636 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/dense/bias/adam_v with shape [768]\n",
      "I0731 21:54:30.910870 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
      "I0731 21:54:32.387574 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0731 21:54:33.905784 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_1/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0731 21:54:35.386249 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
      "I0731 21:54:36.233879 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 21:54:37.044582 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 21:54:37.809055 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0731 21:54:38.612933 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 21:54:39.456862 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 21:54:40.101230 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
      "I0731 21:54:40.984541 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0731 21:54:41.770697 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0731 21:54:42.447060 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
      "I0731 21:54:43.422182 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:54:44.253088 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:54:45.143724 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
      "I0731 21:54:45.998736 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/key/bias/adam_m with shape [768]\n",
      "I0731 21:54:46.646829 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/key/bias/adam_v with shape [768]\n",
      "I0731 21:54:47.529816 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
      "I0731 21:54:48.413182 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:54:49.222377 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:54:50.181047 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
      "I0731 21:54:51.024922 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/query/bias/adam_m with shape [768]\n",
      "I0731 21:54:51.708726 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/query/bias/adam_v with shape [768]\n",
      "I0731 21:54:52.503201 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
      "I0731 21:54:53.318548 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:54:54.312918 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:54:55.174058 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
      "I0731 21:54:55.897762 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/value/bias/adam_m with shape [768]\n",
      "I0731 21:54:56.670212 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/value/bias/adam_v with shape [768]\n",
      "I0731 21:54:57.553630 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
      "I0731 21:54:58.302720 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:54:59.249887 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:55:00.156849 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
      "I0731 21:55:00.880908 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0731 21:55:01.645285 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0731 21:55:02.439883 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0731 21:55:03.828263 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0731 21:55:05.350392 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0731 21:55:06.870409 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
      "I0731 21:55:07.674614 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 21:55:08.518102 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 21:55:09.242117 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
      "I0731 21:55:10.046078 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 21:55:10.850067 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 21:55:11.577414 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n",
      "I0731 21:55:12.421166 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/dense/bias/adam_m with shape [768]\n",
      "I0731 21:55:13.229241 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/dense/bias/adam_v with shape [768]\n",
      "I0731 21:55:13.913930 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
      "I0731 21:55:15.479471 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0731 21:55:16.962150 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_10/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0731 21:55:18.529399 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
      "I0731 21:55:19.333835 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 21:55:20.177542 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 21:55:20.901389 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0731 21:55:21.705359 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 21:55:22.549250 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 21:55:23.233369 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
      "I0731 21:55:24.081137 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0731 21:55:25.001254 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0731 21:55:25.724988 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
      "I0731 21:55:26.583004 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:55:27.537269 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:55:28.278527 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
      "I0731 21:55:29.083256 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/key/bias/adam_m with shape [768]\n",
      "I0731 21:55:29.926373 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/key/bias/adam_v with shape [768]\n",
      "I0731 21:55:30.650605 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
      "I0731 21:55:31.557026 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:55:32.427093 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:55:33.257912 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
      "I0731 21:55:34.101655 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/query/bias/adam_m with shape [768]\n",
      "I0731 21:55:34.872153 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/query/bias/adam_v with shape [768]\n",
      "I0731 21:55:35.597849 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
      "I0731 21:55:36.494286 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:55:37.441587 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:55:38.178568 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
      "I0731 21:55:39.023209 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/value/bias/adam_m with shape [768]\n",
      "I0731 21:55:39.826429 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/value/bias/adam_v with shape [768]\n",
      "I0731 21:55:40.511120 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
      "I0731 21:55:41.461914 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:55:42.394221 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:55:43.181640 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
      "I0731 21:55:44.025629 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0731 21:55:44.829331 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0731 21:55:45.513781 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0731 21:55:47.081168 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0731 21:55:48.601210 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0731 21:55:50.085407 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
      "I0731 21:55:50.968808 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 21:55:51.772734 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 21:55:52.447270 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
      "I0731 21:55:53.198862 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 21:55:53.875352 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 21:55:54.588035 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n",
      "I0731 21:55:55.303046 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/dense/bias/adam_m with shape [768]\n",
      "I0731 21:55:56.094963 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/dense/bias/adam_v with shape [768]\n",
      "I0731 21:55:56.887257 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
      "I0731 21:55:58.249119 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0731 21:55:59.769157 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_11/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0731 21:56:01.328612 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
      "I0731 21:56:02.139088 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 21:56:02.894980 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 21:56:03.579987 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0731 21:56:04.335196 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 21:56:05.060765 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 21:56:05.779478 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
      "I0731 21:56:06.571849 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0731 21:56:07.339416 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0731 21:56:08.136055 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
      "I0731 21:56:08.874223 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:56:09.820992 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:56:10.694487 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
      "I0731 21:56:11.422177 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/key/bias/adam_m with shape [768]\n",
      "I0731 21:56:12.225850 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/key/bias/adam_v with shape [768]\n",
      "I0731 21:56:13.030146 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
      "I0731 21:56:13.811026 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:56:14.718543 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:56:15.664927 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
      "I0731 21:56:16.388888 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/query/bias/adam_m with shape [768]\n",
      "I0731 21:56:17.308815 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/query/bias/adam_v with shape [768]\n",
      "I0731 21:56:18.106708 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
      "I0731 21:56:18.849968 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:56:19.761795 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:56:20.674628 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
      "I0731 21:56:21.402652 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/value/bias/adam_m with shape [768]\n",
      "I0731 21:56:22.166603 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/value/bias/adam_v with shape [768]\n",
      "I0731 21:56:23.010415 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
      "I0731 21:56:23.852775 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:56:24.762094 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:56:25.669090 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
      "I0731 21:56:26.353557 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0731 21:56:27.196970 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0731 21:56:28.156727 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0731 21:56:29.467044 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0731 21:56:30.986851 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0731 21:56:32.506412 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
      "I0731 21:56:33.428197 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 21:56:34.229887 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 21:56:34.969650 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
      "I0731 21:56:35.773394 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 21:56:36.617300 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 21:56:37.301534 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n",
      "I0731 21:56:38.149099 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/dense/bias/adam_m with shape [768]\n",
      "I0731 21:56:38.957096 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/dense/bias/adam_v with shape [768]\n",
      "I0731 21:56:39.685050 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
      "I0731 21:56:41.125609 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0731 21:56:42.645267 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_2/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0731 21:56:44.208874 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
      "I0731 21:56:44.963471 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 21:56:45.599573 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 21:56:46.430955 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0731 21:56:47.028634 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 21:56:47.862932 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 21:56:48.498861 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
      "I0731 21:56:49.299347 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0731 21:56:49.935197 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0731 21:56:50.726832 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
      "I0731 21:56:51.466066 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:56:52.416987 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:56:53.290575 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
      "I0731 21:56:54.014750 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/key/bias/adam_m with shape [768]\n",
      "I0731 21:56:54.938458 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/key/bias/adam_v with shape [768]\n",
      "I0731 21:56:55.781974 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
      "I0731 21:56:56.572760 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:56:57.490052 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:56:58.357815 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
      "I0731 21:56:59.121431 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/query/bias/adam_m with shape [768]\n",
      "I0731 21:56:59.889312 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/query/bias/adam_v with shape [768]\n",
      "I0731 21:57:00.772823 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
      "I0731 21:57:01.470640 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:57:02.497035 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:57:03.386429 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
      "I0731 21:57:04.114325 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/value/bias/adam_m with shape [768]\n",
      "I0731 21:57:04.998558 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/value/bias/adam_v with shape [768]\n",
      "I0731 21:57:05.842245 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
      "I0731 21:57:06.619985 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:57:07.506632 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:57:08.413540 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
      "I0731 21:57:09.137428 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0731 21:57:09.895857 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0731 21:57:10.611095 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0731 21:57:12.061216 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0731 21:57:13.620800 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0731 21:57:15.151376 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
      "I0731 21:57:15.946707 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 21:57:16.583199 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 21:57:17.343686 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
      "I0731 21:57:18.018777 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 21:57:18.810783 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 21:57:19.411226 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n",
      "I0731 21:57:20.242796 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/dense/bias/adam_m with shape [768]\n",
      "I0731 21:57:20.878738 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/dense/bias/adam_v with shape [768]\n",
      "I0731 21:57:21.635388 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
      "I0731 21:57:22.986558 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0731 21:57:24.467090 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_3/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0731 21:57:26.070810 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
      "I0731 21:57:26.914644 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 21:57:27.758037 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 21:57:28.481933 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0731 21:57:29.250118 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 21:57:30.094081 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 21:57:30.817317 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
      "I0731 21:57:31.665209 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0731 21:57:32.473163 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0731 21:57:33.197048 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
      "I0731 21:57:34.054850 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:57:35.001629 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:57:35.739049 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
      "I0731 21:57:36.654928 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/key/bias/adam_m with shape [768]\n",
      "I0731 21:57:37.407055 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/key/bias/adam_v with shape [768]\n",
      "I0731 21:57:38.083061 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
      "I0731 21:57:38.942094 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:57:39.733910 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:57:40.606870 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
      "I0731 21:57:41.490318 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/query/bias/adam_m with shape [768]\n",
      "I0731 21:57:42.174319 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/query/bias/adam_v with shape [768]\n",
      "I0731 21:57:43.026022 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
      "I0731 21:57:43.936671 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:57:44.724478 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:57:45.593753 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
      "I0731 21:57:46.484252 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/value/bias/adam_m with shape [768]\n",
      "I0731 21:57:47.209624 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/value/bias/adam_v with shape [768]\n",
      "I0731 21:57:48.043531 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
      "I0731 21:57:48.742859 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:57:49.729289 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:57:50.598593 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
      "I0731 21:57:51.287004 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0731 21:57:52.130292 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0731 21:57:52.974305 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0731 21:57:54.341861 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0731 21:57:55.822110 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0731 21:57:57.354056 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
      "I0731 21:57:58.241798 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 21:57:59.006103 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 21:57:59.773561 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
      "I0731 21:58:00.538121 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 21:58:01.429600 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 21:58:02.117326 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n",
      "I0731 21:58:02.961203 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/dense/bias/adam_m with shape [768]\n",
      "I0731 21:58:03.776934 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/dense/bias/adam_v with shape [768]\n",
      "I0731 21:58:04.461836 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
      "I0731 21:58:05.915309 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0731 21:58:07.551968 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_4/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0731 21:58:09.070567 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
      "I0731 21:58:09.914457 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 21:58:10.678666 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 21:58:11.441992 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0731 21:58:12.249701 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 21:58:13.054162 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 21:58:13.737927 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
      "I0731 21:58:14.581789 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0731 21:58:15.425028 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0731 21:58:16.149049 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
      "I0731 21:58:17.006407 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:58:17.929534 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:58:18.710353 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
      "I0731 21:58:19.555495 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/key/bias/adam_m with shape [768]\n",
      "I0731 21:58:20.318723 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/key/bias/adam_v with shape [768]\n",
      "I0731 21:58:21.089975 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
      "I0731 21:58:21.946960 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:58:22.894251 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:58:23.645543 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
      "I0731 21:58:24.537074 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/query/bias/adam_m with shape [768]\n",
      "I0731 21:58:25.352931 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/query/bias/adam_v with shape [768]\n",
      "I0731 21:58:26.037691 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
      "I0731 21:58:26.934410 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:58:27.842209 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:58:28.696499 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
      "I0731 21:58:29.539266 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/value/bias/adam_m with shape [768]\n",
      "I0731 21:58:30.386463 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/value/bias/adam_v with shape [768]\n",
      "I0731 21:58:31.110522 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
      "I0731 21:58:32.017531 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:58:32.930497 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:58:33.837805 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
      "I0731 21:58:34.681632 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0731 21:58:35.485563 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0731 21:58:36.209308 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0731 21:58:37.689192 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0731 21:58:39.244843 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0731 21:58:40.731179 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
      "I0731 21:58:41.491574 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 21:58:42.127483 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 21:58:42.924180 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
      "I0731 21:58:43.559949 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 21:58:44.471603 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 21:58:45.227339 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n",
      "I0731 21:58:45.903125 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/dense/bias/adam_m with shape [768]\n",
      "I0731 21:58:46.659093 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/dense/bias/adam_v with shape [768]\n",
      "I0731 21:58:47.334762 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
      "I0731 21:58:48.762926 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0731 21:58:50.290519 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_5/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0731 21:58:51.810198 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
      "I0731 21:58:52.658119 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 21:58:53.461862 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 21:58:54.185711 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0731 21:58:55.070046 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 21:58:55.953504 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 21:58:56.601989 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
      "I0731 21:58:57.489368 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0731 21:58:58.293240 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0731 21:58:58.975278 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
      "I0731 21:58:59.834292 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:59:00.813256 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:59:01.578470 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
      "I0731 21:59:02.431471 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/key/bias/adam_m with shape [768]\n",
      "I0731 21:59:03.238237 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/key/bias/adam_v with shape [768]\n",
      "I0731 21:59:03.977951 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
      "I0731 21:59:04.874168 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:59:05.822616 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:59:06.653235 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
      "I0731 21:59:07.501093 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/query/bias/adam_m with shape [768]\n",
      "I0731 21:59:08.305102 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/query/bias/adam_v with shape [768]\n",
      "I0731 21:59:08.983254 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
      "I0731 21:59:09.842323 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:59:10.753887 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:59:11.559108 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
      "I0731 21:59:12.402446 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/value/bias/adam_m with shape [768]\n",
      "I0731 21:59:13.206475 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/value/bias/adam_v with shape [768]\n",
      "I0731 21:59:13.934106 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
      "I0731 21:59:14.801455 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:59:15.874435 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:59:16.661041 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
      "I0731 21:59:17.465681 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0731 21:59:18.309154 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0731 21:59:19.017422 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0731 21:59:20.487519 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0731 21:59:22.006742 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0731 21:59:23.526719 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
      "I0731 21:59:24.331014 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 21:59:25.134633 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 21:59:26.013946 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
      "I0731 21:59:26.818313 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 21:59:27.661990 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 21:59:28.306110 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n",
      "I0731 21:59:29.193577 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/dense/bias/adam_m with shape [768]\n",
      "I0731 21:59:29.997431 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/dense/bias/adam_v with shape [768]\n",
      "I0731 21:59:30.721310 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
      "I0731 21:59:32.200865 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0731 21:59:33.670917 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_6/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0731 21:59:35.194512 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
      "I0731 21:59:36.116388 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 21:59:36.938989 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 21:59:37.690352 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0731 21:59:38.494335 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 21:59:39.353996 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 21:59:40.006223 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
      "I0731 21:59:40.850008 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0731 21:59:41.693641 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0731 21:59:42.429483 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
      "I0731 21:59:43.287172 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:59:44.194438 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:59:44.971122 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
      "I0731 21:59:45.762874 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/key/bias/adam_m with shape [768]\n",
      "I0731 21:59:46.438123 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/key/bias/adam_v with shape [768]\n",
      "I0731 21:59:47.271138 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
      "I0731 21:59:48.010138 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:59:48.921936 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:59:49.838793 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
      "I0731 21:59:50.523380 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/query/bias/adam_m with shape [768]\n",
      "I0731 21:59:51.326894 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/query/bias/adam_v with shape [768]\n",
      "I0731 21:59:52.170524 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
      "I0731 21:59:52.957373 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:59:53.886837 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:59:54.889363 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
      "I0731 21:59:55.613124 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/value/bias/adam_m with shape [768]\n",
      "I0731 21:59:56.417188 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/value/bias/adam_v with shape [768]\n",
      "I0731 21:59:57.337784 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
      "I0731 21:59:58.118560 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0731 21:59:59.065485 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0731 21:59:59.895274 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
      "I0731 22:00:00.619205 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0731 22:00:01.426928 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0731 22:00:02.310410 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0731 22:00:03.677779 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0731 22:00:05.158040 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0731 22:00:06.678262 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
      "I0731 22:00:07.681346 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 22:00:08.489089 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 22:00:09.213135 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
      "I0731 22:00:09.975480 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 22:00:10.770979 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 22:00:11.407222 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n",
      "I0731 22:00:12.203356 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/dense/bias/adam_m with shape [768]\n",
      "I0731 22:00:12.839530 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/dense/bias/adam_v with shape [768]\n",
      "I0731 22:00:13.596042 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
      "I0731 22:00:14.997032 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0731 22:00:16.480739 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_7/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0731 22:00:18.003122 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
      "I0731 22:00:18.850618 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 22:00:19.659064 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 22:00:20.382542 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0731 22:00:21.186718 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 22:00:22.030210 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 22:00:22.678706 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
      "I0731 22:00:23.561668 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0731 22:00:24.365600 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0731 22:00:25.093654 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
      "I0731 22:00:25.911334 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0731 22:00:26.898551 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0731 22:00:27.689243 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
      "I0731 22:00:28.603373 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/key/bias/adam_m with shape [768]\n",
      "I0731 22:00:29.359551 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/key/bias/adam_v with shape [768]\n",
      "I0731 22:00:30.035059 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
      "I0731 22:00:30.894836 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0731 22:00:31.721587 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0731 22:00:32.551484 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
      "I0731 22:00:33.439694 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/query/bias/adam_m with shape [768]\n",
      "I0731 22:00:34.127480 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/query/bias/adam_v with shape [768]\n",
      "I0731 22:00:34.971456 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
      "I0731 22:00:35.842235 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0731 22:00:36.643435 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0731 22:00:37.670577 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
      "I0731 22:00:38.553855 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/value/bias/adam_m with shape [768]\n",
      "I0731 22:00:39.237674 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/value/bias/adam_v with shape [768]\n",
      "I0731 22:00:40.089860 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
      "I0731 22:00:40.970648 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0731 22:00:41.797974 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0731 22:00:42.615784 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
      "I0731 22:00:43.459759 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0731 22:00:44.143354 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0731 22:00:45.026304 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0731 22:00:46.466911 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0731 22:00:48.026565 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0731 22:00:49.546080 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
      "I0731 22:00:50.389686 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 22:00:51.193676 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 22:00:51.878047 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
      "I0731 22:00:52.682373 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 22:00:53.530091 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 22:00:54.253432 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n",
      "I0731 22:00:55.098264 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/dense/bias/adam_m with shape [768]\n",
      "I0731 22:00:55.865613 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/dense/bias/adam_v with shape [768]\n",
      "I0731 22:00:56.539637 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
      "I0731 22:00:58.084113 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0731 22:00:59.602833 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_8/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0731 22:01:01.126631 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
      "I0731 22:01:01.986599 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 22:01:02.794370 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 22:01:03.522465 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
      "I0731 22:01:04.286297 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 22:01:05.174102 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 22:01:05.861855 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
      "I0731 22:01:06.705676 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/dense/bias/adam_m with shape [768]\n",
      "I0731 22:01:07.521583 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/dense/bias/adam_v with shape [768]\n",
      "I0731 22:01:08.288949 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
      "I0731 22:01:09.187351 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
      "I0731 22:01:10.133920 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
      "I0731 22:01:10.875424 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
      "I0731 22:01:11.671592 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/key/bias/adam_m with shape [768]\n",
      "I0731 22:01:12.307123 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/key/bias/adam_v with shape [768]\n",
      "I0731 22:01:13.099328 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
      "I0731 22:01:13.838527 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/key/kernel/adam_m with shape [768, 768]\n",
      "I0731 22:01:14.853338 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/key/kernel/adam_v with shape [768, 768]\n",
      "I0731 22:01:15.742781 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
      "I0731 22:01:16.478452 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/query/bias/adam_m with shape [768]\n",
      "I0731 22:01:17.282037 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/query/bias/adam_v with shape [768]\n",
      "I0731 22:01:18.126130 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
      "I0731 22:01:18.877527 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/query/kernel/adam_m with shape [768, 768]\n",
      "I0731 22:01:19.846586 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/query/kernel/adam_v with shape [768, 768]\n",
      "I0731 22:01:20.753682 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
      "I0731 22:01:21.438041 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/value/bias/adam_m with shape [768]\n",
      "I0731 22:01:22.241729 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/value/bias/adam_v with shape [768]\n",
      "I0731 22:01:23.085775 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
      "I0731 22:01:23.837224 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/value/kernel/adam_m with shape [768, 768]\n",
      "I0731 22:01:24.825650 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/attention/self/value/kernel/adam_v with shape [768, 768]\n",
      "I0731 22:01:25.663298 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
      "I0731 22:01:26.426887 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/intermediate/dense/bias/adam_m with shape [3072]\n",
      "I0731 22:01:27.230904 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/intermediate/dense/bias/adam_v with shape [3072]\n",
      "I0731 22:01:28.035382 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
      "I0731 22:01:29.434610 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "I0731 22:01:30.958469 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "I0731 22:01:32.477900 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
      "I0731 22:01:33.321664 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 22:01:34.125686 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 22:01:34.853759 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
      "I0731 22:01:35.657549 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 22:01:36.466024 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 22:01:37.189357 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n",
      "I0731 22:01:38.033252 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/dense/bias/adam_m with shape [768]\n",
      "I0731 22:01:38.747854 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/dense/bias/adam_v with shape [768]\n",
      "I0731 22:01:39.500673 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
      "I0731 22:01:41.027651 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "I0731 22:01:42.499037 139833534199616 modeling_bert.py:90] Loading TF weight bert/encoder/layer_9/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "I0731 22:01:44.022621 139833534199616 modeling_bert.py:90] Loading TF weight bert/pooler/dense/bias with shape [768]\n",
      "I0731 22:01:44.866488 139833534199616 modeling_bert.py:90] Loading TF weight bert/pooler/dense/bias/adam_m with shape [768]\n",
      "I0731 22:01:45.670525 139833534199616 modeling_bert.py:90] Loading TF weight bert/pooler/dense/bias/adam_v with shape [768]\n",
      "I0731 22:01:46.398265 139833534199616 modeling_bert.py:90] Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
      "I0731 22:01:47.304902 139833534199616 modeling_bert.py:90] Loading TF weight bert/pooler/dense/kernel/adam_m with shape [768, 768]\n",
      "I0731 22:01:48.238689 139833534199616 modeling_bert.py:90] Loading TF weight bert/pooler/dense/kernel/adam_v with shape [768, 768]\n",
      "I0731 22:01:49.029609 139833534199616 modeling_bert.py:90] Loading TF weight cls/predictions/output_bias with shape [30522]\n",
      "I0731 22:01:49.873576 139833534199616 modeling_bert.py:90] Loading TF weight cls/predictions/output_bias/adam_m with shape [30522]\n",
      "I0731 22:01:50.797279 139833534199616 modeling_bert.py:90] Loading TF weight cls/predictions/output_bias/adam_v with shape [30522]\n",
      "I0731 22:01:51.521092 139833534199616 modeling_bert.py:90] Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [768]\n",
      "I0731 22:01:52.325063 139833534199616 modeling_bert.py:90] Loading TF weight cls/predictions/transform/LayerNorm/beta/adam_m with shape [768]\n",
      "I0731 22:01:53.119123 139833534199616 modeling_bert.py:90] Loading TF weight cls/predictions/transform/LayerNorm/beta/adam_v with shape [768]\n",
      "I0731 22:01:53.755333 139833534199616 modeling_bert.py:90] Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [768]\n",
      "I0731 22:01:54.551196 139833534199616 modeling_bert.py:90] Loading TF weight cls/predictions/transform/LayerNorm/gamma/adam_m with shape [768]\n",
      "I0731 22:01:55.187091 139833534199616 modeling_bert.py:90] Loading TF weight cls/predictions/transform/LayerNorm/gamma/adam_v with shape [768]\n",
      "I0731 22:01:55.979243 139833534199616 modeling_bert.py:90] Loading TF weight cls/predictions/transform/dense/bias with shape [768]\n",
      "I0731 22:01:56.615686 139833534199616 modeling_bert.py:90] Loading TF weight cls/predictions/transform/dense/bias/adam_m with shape [768]\n",
      "I0731 22:01:57.411234 139833534199616 modeling_bert.py:90] Loading TF weight cls/predictions/transform/dense/bias/adam_v with shape [768]\n",
      "I0731 22:01:58.007584 139833534199616 modeling_bert.py:90] Loading TF weight cls/predictions/transform/dense/kernel with shape [768, 768]\n",
      "I0731 22:01:58.902834 139833534199616 modeling_bert.py:90] Loading TF weight cls/predictions/transform/dense/kernel/adam_m with shape [768, 768]\n",
      "I0731 22:01:59.853461 139833534199616 modeling_bert.py:90] Loading TF weight cls/predictions/transform/dense/kernel/adam_v with shape [768, 768]\n",
      "I0731 22:02:00.625884 139833534199616 modeling_bert.py:90] Loading TF weight cls/seq_relationship/output_bias with shape [2]\n",
      "I0731 22:02:01.261570 139833534199616 modeling_bert.py:90] Loading TF weight cls/seq_relationship/output_bias/adam_m with shape [2]\n",
      "I0731 22:02:01.818791 139833534199616 modeling_bert.py:90] Loading TF weight cls/seq_relationship/output_bias/adam_v with shape [2]\n",
      "I0731 22:02:02.467992 139833534199616 modeling_bert.py:90] Loading TF weight cls/seq_relationship/output_weights with shape [2, 768]\n",
      "I0731 22:02:03.027986 139833534199616 modeling_bert.py:90] Loading TF weight cls/seq_relationship/output_weights/adam_m with shape [2, 768]\n",
      "I0731 22:02:03.663196 139833534199616 modeling_bert.py:90] Loading TF weight cls/seq_relationship/output_weights/adam_v with shape [2, 768]\n",
      "I0731 22:02:04.243097 139833534199616 modeling_bert.py:90] Loading TF weight global_step with shape []\n",
      "I0731 22:02:04.885138 139833534199616 modeling_bert.py:90] Loading TF weight good_steps with shape []\n",
      "I0731 22:02:05.442181 139833534199616 modeling_bert.py:90] Loading TF weight loss_scale with shape []\n",
      "I0731 22:02:06.077672 139833534199616 modeling_bert.py:100] Skipping bad_steps\n",
      "FinBERT Converting: ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.078235 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.078430 139833534199616 modeling_bert.py:100] Skipping bert/embeddings/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.078510 139833534199616 modeling_bert.py:100] Skipping bert/embeddings/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.078608 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.078682 139833534199616 modeling_bert.py:100] Skipping bert/embeddings/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.078740 139833534199616 modeling_bert.py:100] Skipping bert/embeddings/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'embeddings', 'position_embeddings']\n",
      "I0731 22:02:06.078824 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
      "I0731 22:02:06.079065 139833534199616 modeling_bert.py:100] Skipping bert/embeddings/position_embeddings/adam_m\n",
      "I0731 22:02:06.079128 139833534199616 modeling_bert.py:100] Skipping bert/embeddings/position_embeddings/adam_v\n",
      "FinBERT Converting: ['bert', 'embeddings', 'token_type_embeddings']\n",
      "I0731 22:02:06.079210 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
      "I0731 22:02:06.079279 139833534199616 modeling_bert.py:100] Skipping bert/embeddings/token_type_embeddings/adam_m\n",
      "I0731 22:02:06.079332 139833534199616 modeling_bert.py:100] Skipping bert/embeddings/token_type_embeddings/adam_v\n",
      "FinBERT Converting: ['bert', 'embeddings', 'word_embeddings']\n",
      "I0731 22:02:06.079409 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
      "I0731 22:02:06.087396 139833534199616 modeling_bert.py:100] Skipping bert/embeddings/word_embeddings/adam_m\n",
      "I0731 22:02:06.087476 139833534199616 modeling_bert.py:100] Skipping bert/embeddings/word_embeddings/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.087746 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.087846 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.087917 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.088036 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.088118 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.088183 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.088302 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.088382 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/output/dense/bias/adam_m\n",
      "I0731 22:02:06.088447 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.088571 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.088855 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.088927 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.089050 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.089132 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/key/bias/adam_m\n",
      "I0731 22:02:06.089195 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.089313 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.089603 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/key/kernel/adam_m\n",
      "I0731 22:02:06.089675 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.089813 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.089897 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/query/bias/adam_m\n",
      "I0731 22:02:06.089962 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.090080 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.090381 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/query/kernel/adam_m\n",
      "I0731 22:02:06.090453 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.090571 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.090651 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/value/bias/adam_m\n",
      "I0731 22:02:06.090716 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.090835 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.091127 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/value/kernel/adam_m\n",
      "I0731 22:02:06.091200 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.091312 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.091393 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/intermediate/dense/bias/adam_m\n",
      "I0731 22:02:06.091451 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.091564 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.092437 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/intermediate/dense/kernel/adam_m\n",
      "I0731 22:02:06.092509 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.092614 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.092686 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.092740 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.092833 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.092899 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.092953 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.093045 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.093111 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/output/dense/bias/adam_m\n",
      "I0731 22:02:06.093174 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.093291 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.094194 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.094270 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_0/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.094412 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.094496 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.094562 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.094670 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.094747 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.094811 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.094924 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.095002 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/output/dense/bias/adam_m\n",
      "I0731 22:02:06.095066 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.095198 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.095494 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.095567 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.095683 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.095763 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/key/bias/adam_m\n",
      "I0731 22:02:06.095827 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.095942 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.096238 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/key/kernel/adam_m\n",
      "I0731 22:02:06.096312 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.096429 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.096524 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/query/bias/adam_m\n",
      "I0731 22:02:06.096587 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.096703 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.097001 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/query/kernel/adam_m\n",
      "I0731 22:02:06.097073 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.097190 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.097283 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/value/bias/adam_m\n",
      "I0731 22:02:06.097421 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.097727 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.098047 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/value/kernel/adam_m\n",
      "I0731 22:02:06.098123 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.098247 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.098334 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/intermediate/dense/bias/adam_m\n",
      "I0731 22:02:06.098398 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.098511 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.099414 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/intermediate/dense/kernel/adam_m\n",
      "I0731 22:02:06.099491 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.099609 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.099692 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.099756 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.099864 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.099941 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.100003 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.100110 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.100186 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/output/dense/bias/adam_m\n",
      "I0731 22:02:06.100250 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.100359 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.101222 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.101299 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_1/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.101424 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.101507 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.101573 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.101688 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.101792 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.101860 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.101974 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.102052 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/output/dense/bias/adam_m\n",
      "I0731 22:02:06.102117 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.102235 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.102504 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.102576 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.102696 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.102777 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/key/bias/adam_m\n",
      "I0731 22:02:06.102841 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.102959 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.103227 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/key/kernel/adam_m\n",
      "I0731 22:02:06.103301 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.103419 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.103498 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/query/bias/adam_m\n",
      "I0731 22:02:06.103562 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.103678 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.103949 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/query/kernel/adam_m\n",
      "I0731 22:02:06.104020 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.104134 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.104214 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/value/bias/adam_m\n",
      "I0731 22:02:06.104277 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.104392 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.104655 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/value/kernel/adam_m\n",
      "I0731 22:02:06.104726 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.104836 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.104915 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/intermediate/dense/bias/adam_m\n",
      "I0731 22:02:06.104978 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.105091 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.105891 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/intermediate/dense/kernel/adam_m\n",
      "I0731 22:02:06.105972 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.106090 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.106172 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.106235 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.106342 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.106419 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.106481 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.106583 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.106657 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/output/dense/bias/adam_m\n",
      "I0731 22:02:06.106719 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.106830 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.107594 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.107667 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_10/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.107788 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.107870 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.107934 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.108045 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.108122 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.108186 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.108296 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.108372 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/output/dense/bias/adam_m\n",
      "I0731 22:02:06.108436 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.108547 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.108812 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.108884 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.109000 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.109079 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/key/bias/adam_m\n",
      "I0731 22:02:06.109142 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.109256 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.109531 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/key/kernel/adam_m\n",
      "I0731 22:02:06.109603 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.109713 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.109816 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/query/bias/adam_m\n",
      "I0731 22:02:06.109882 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.109999 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.110262 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/query/kernel/adam_m\n",
      "I0731 22:02:06.110334 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.110458 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.110536 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/value/bias/adam_m\n",
      "I0731 22:02:06.110601 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.110718 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.110985 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/value/kernel/adam_m\n",
      "I0731 22:02:06.111057 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.111171 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.111251 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/intermediate/dense/bias/adam_m\n",
      "I0731 22:02:06.111315 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.111430 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.112217 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/intermediate/dense/kernel/adam_m\n",
      "I0731 22:02:06.112291 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.112409 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.112489 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.112553 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.112659 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.112734 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.112795 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.112900 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.112975 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/output/dense/bias/adam_m\n",
      "I0731 22:02:06.113037 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.113144 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.113959 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.114034 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_11/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.114158 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.114241 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.114305 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.114417 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.114495 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.114559 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.114673 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.114750 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/output/dense/bias/adam_m\n",
      "I0731 22:02:06.114814 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.114925 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.115200 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.115272 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.115391 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.115471 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/key/bias/adam_m\n",
      "I0731 22:02:06.115536 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.115654 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.115924 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/key/kernel/adam_m\n",
      "I0731 22:02:06.115996 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.116112 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.116192 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/query/bias/adam_m\n",
      "I0731 22:02:06.116255 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.116370 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.116637 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/query/kernel/adam_m\n",
      "I0731 22:02:06.116710 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.116825 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.116897 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/value/bias/adam_m\n",
      "I0731 22:02:06.116961 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.117076 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.117351 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/value/kernel/adam_m\n",
      "I0731 22:02:06.117423 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.117533 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.117612 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/intermediate/dense/bias/adam_m\n",
      "I0731 22:02:06.117676 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.117844 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.118640 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/intermediate/dense/kernel/adam_m\n",
      "I0731 22:02:06.118715 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.118834 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.118915 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.118981 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.119091 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.119168 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.119232 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.119333 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.119410 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/output/dense/bias/adam_m\n",
      "I0731 22:02:06.119474 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.119587 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.120375 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.120456 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_2/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.120580 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.120662 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.120727 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.120839 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.120917 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.120982 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.121093 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.121170 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/output/dense/bias/adam_m\n",
      "I0731 22:02:06.121235 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.121352 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.121615 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.121688 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.121857 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.121940 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/key/bias/adam_m\n",
      "I0731 22:02:06.122004 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.122121 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.122392 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/key/kernel/adam_m\n",
      "I0731 22:02:06.122463 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.122580 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.122659 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/query/bias/adam_m\n",
      "I0731 22:02:06.122723 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.122842 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.123112 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/query/kernel/adam_m\n",
      "I0731 22:02:06.123185 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.123301 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.123382 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/value/bias/adam_m\n",
      "I0731 22:02:06.123445 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.123555 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.123826 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/value/kernel/adam_m\n",
      "I0731 22:02:06.123898 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.124011 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.124088 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/intermediate/dense/bias/adam_m\n",
      "I0731 22:02:06.124150 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.124264 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.125047 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/intermediate/dense/kernel/adam_m\n",
      "I0731 22:02:06.125121 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.125236 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.125315 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.125379 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.125486 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.125562 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.125624 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.125731 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.125831 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/output/dense/bias/adam_m\n",
      "I0731 22:02:06.125895 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.126008 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.126780 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.126853 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_3/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.126980 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.127062 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.127127 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.127242 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.127319 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.127383 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.127490 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.127566 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/output/dense/bias/adam_m\n",
      "I0731 22:02:06.127628 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.127750 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.128020 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.128093 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.128212 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.128292 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/key/bias/adam_m\n",
      "I0731 22:02:06.128357 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.128473 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.128741 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/key/kernel/adam_m\n",
      "I0731 22:02:06.128813 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.128930 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.129009 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/query/bias/adam_m\n",
      "I0731 22:02:06.129072 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.129188 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.129452 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/query/kernel/adam_m\n",
      "I0731 22:02:06.129525 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.129641 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.129720 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/value/bias/adam_m\n",
      "I0731 22:02:06.129806 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.129925 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.130188 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/value/kernel/adam_m\n",
      "I0731 22:02:06.130259 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.130374 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.130461 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/intermediate/dense/bias/adam_m\n",
      "I0731 22:02:06.130527 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.130641 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.131424 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/intermediate/dense/kernel/adam_m\n",
      "I0731 22:02:06.131509 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.131627 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.131708 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.131772 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.131879 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.131954 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.132017 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.132125 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.132200 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/output/dense/bias/adam_m\n",
      "I0731 22:02:06.132263 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.132374 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.133173 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.133257 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_4/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.133381 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.133464 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.133528 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.133639 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.133717 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.133805 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.133921 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.133999 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/output/dense/bias/adam_m\n",
      "I0731 22:02:06.134064 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.134181 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.134447 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.134529 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.134649 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.134729 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/key/bias/adam_m\n",
      "I0731 22:02:06.134794 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.134909 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.135182 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/key/kernel/adam_m\n",
      "I0731 22:02:06.135256 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.135373 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.135452 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/query/bias/adam_m\n",
      "I0731 22:02:06.135517 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.135633 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.135902 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/query/kernel/adam_m\n",
      "I0731 22:02:06.135984 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.136101 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.136180 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/value/bias/adam_m\n",
      "I0731 22:02:06.136243 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.136357 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.136625 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/value/kernel/adam_m\n",
      "I0731 22:02:06.136697 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.136813 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.136893 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/intermediate/dense/bias/adam_m\n",
      "I0731 22:02:06.136956 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.137068 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.137864 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/intermediate/dense/kernel/adam_m\n",
      "I0731 22:02:06.137948 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.138065 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.138147 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.138223 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.138332 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.138410 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.138474 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.138580 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.138655 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/output/dense/bias/adam_m\n",
      "I0731 22:02:06.138729 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.138844 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.139646 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.139733 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_5/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.139858 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.139943 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.140011 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.140123 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.140200 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.140263 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.140373 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.140458 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/output/dense/bias/adam_m\n",
      "I0731 22:02:06.140524 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.140640 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.140906 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.140978 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.141096 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.141175 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/key/bias/adam_m\n",
      "I0731 22:02:06.141240 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.141356 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.141657 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/key/kernel/adam_m\n",
      "I0731 22:02:06.141733 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.141899 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.141983 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/query/bias/adam_m\n",
      "I0731 22:02:06.142047 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.142165 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.142462 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/query/kernel/adam_m\n",
      "I0731 22:02:06.142537 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.142682 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.142766 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/value/bias/adam_m\n",
      "I0731 22:02:06.142831 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.142949 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.143223 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/value/kernel/adam_m\n",
      "I0731 22:02:06.143297 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.143441 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.143528 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/intermediate/dense/bias/adam_m\n",
      "I0731 22:02:06.143592 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.143705 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.144490 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/intermediate/dense/kernel/adam_m\n",
      "I0731 22:02:06.144564 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.144681 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.144762 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.144827 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.144933 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.145009 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.145073 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.145179 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.145256 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/output/dense/bias/adam_m\n",
      "I0731 22:02:06.145319 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.145431 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.146247 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.146336 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_6/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.146464 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.146548 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.146613 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.146726 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.146803 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.146867 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.146978 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.147056 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/output/dense/bias/adam_m\n",
      "I0731 22:02:06.147120 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.147237 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.147505 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.147588 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.147707 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.147789 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/key/bias/adam_m\n",
      "I0731 22:02:06.147853 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.147969 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.148239 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/key/kernel/adam_m\n",
      "I0731 22:02:06.148312 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.148429 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.148523 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/query/bias/adam_m\n",
      "I0731 22:02:06.148588 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.148704 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.148977 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/query/kernel/adam_m\n",
      "I0731 22:02:06.149049 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.149163 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.149242 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/value/bias/adam_m\n",
      "I0731 22:02:06.149305 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.149419 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.149693 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/value/kernel/adam_m\n",
      "I0731 22:02:06.149794 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.149944 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.150026 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/intermediate/dense/bias/adam_m\n",
      "I0731 22:02:06.150090 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.150205 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.150972 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/intermediate/dense/kernel/adam_m\n",
      "I0731 22:02:06.151059 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.151206 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.151639 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.152090 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.152298 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.152383 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.152448 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.152556 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.152633 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/output/dense/bias/adam_m\n",
      "I0731 22:02:06.152697 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.152810 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.153594 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.153678 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_7/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.153830 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.153916 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.153982 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.154093 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.154170 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.154233 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.154355 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.154433 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/output/dense/bias/adam_m\n",
      "I0731 22:02:06.154497 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.154615 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.154887 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.154968 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.155089 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.155169 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/key/bias/adam_m\n",
      "I0731 22:02:06.155234 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.155348 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.155618 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/key/kernel/adam_m\n",
      "I0731 22:02:06.155691 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.155808 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.155887 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/query/bias/adam_m\n",
      "I0731 22:02:06.155951 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.156067 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.156334 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/query/kernel/adam_m\n",
      "I0731 22:02:06.156418 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.156538 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.156617 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/value/bias/adam_m\n",
      "I0731 22:02:06.156682 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.156796 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.157068 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/value/kernel/adam_m\n",
      "I0731 22:02:06.157141 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.157255 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.157334 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/intermediate/dense/bias/adam_m\n",
      "I0731 22:02:06.157397 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.157508 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.158307 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/intermediate/dense/kernel/adam_m\n",
      "I0731 22:02:06.158393 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.158514 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.158597 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.158663 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.158770 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.158848 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.158912 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.159018 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.159105 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/output/dense/bias/adam_m\n",
      "I0731 22:02:06.159168 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.159280 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.160047 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.160132 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_8/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.160257 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.160339 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.160409 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.160523 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.160601 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.160664 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.160775 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.160851 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/output/dense/bias/adam_m\n",
      "I0731 22:02:06.160914 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.161032 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.161289 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.161364 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.161482 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
      "I0731 22:02:06.161560 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/key/bias/adam_m\n",
      "I0731 22:02:06.161623 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/key/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.161779 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
      "I0731 22:02:06.162049 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/key/kernel/adam_m\n",
      "I0731 22:02:06.162123 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/key/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.162240 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
      "I0731 22:02:06.162320 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/query/bias/adam_m\n",
      "I0731 22:02:06.162384 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/query/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.162499 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
      "I0731 22:02:06.162769 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/query/kernel/adam_m\n",
      "I0731 22:02:06.162842 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/query/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.162957 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
      "I0731 22:02:06.163036 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/value/bias/adam_m\n",
      "I0731 22:02:06.163100 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/value/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.163216 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
      "I0731 22:02:06.163480 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/value/kernel/adam_m\n",
      "I0731 22:02:06.163553 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/attention/self/value/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.163667 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
      "I0731 22:02:06.163744 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/intermediate/dense/bias/adam_m\n",
      "I0731 22:02:06.163808 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/intermediate/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.163921 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
      "I0731 22:02:06.164692 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/intermediate/dense/kernel/adam_m\n",
      "I0731 22:02:06.164775 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/intermediate/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.164892 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.164973 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/output/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.165036 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/output/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.165142 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.165219 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/output/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.165282 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/output/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.165389 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
      "I0731 22:02:06.165464 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/output/dense/bias/adam_m\n",
      "I0731 22:02:06.165527 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/output/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.165639 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
      "I0731 22:02:06.166406 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/output/dense/kernel/adam_m\n",
      "I0731 22:02:06.166492 139833534199616 modeling_bert.py:100] Skipping bert/encoder/layer_9/output/dense/kernel/adam_v\n",
      "FinBERT Converting: ['bert', 'pooler', 'dense', 'bias']\n",
      "I0731 22:02:06.166596 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
      "I0731 22:02:06.166673 139833534199616 modeling_bert.py:100] Skipping bert/pooler/dense/bias/adam_m\n",
      "I0731 22:02:06.166736 139833534199616 modeling_bert.py:100] Skipping bert/pooler/dense/bias/adam_v\n",
      "FinBERT Converting: ['bert', 'pooler', 'dense', 'kernel']\n",
      "I0731 22:02:06.166831 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
      "I0731 22:02:06.167089 139833534199616 modeling_bert.py:100] Skipping bert/pooler/dense/kernel/adam_m\n",
      "I0731 22:02:06.167170 139833534199616 modeling_bert.py:100] Skipping bert/pooler/dense/kernel/adam_v\n",
      "FinBERT Converting: ['cls', 'predictions', 'output_bias']\n",
      "I0731 22:02:06.167269 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['cls', 'predictions', 'output_bias']\n",
      "I0731 22:02:06.167346 139833534199616 modeling_bert.py:100] Skipping cls/predictions/output_bias/adam_m\n",
      "I0731 22:02:06.167409 139833534199616 modeling_bert.py:100] Skipping cls/predictions/output_bias/adam_v\n",
      "FinBERT Converting: ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.167509 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\n",
      "I0731 22:02:06.167584 139833534199616 modeling_bert.py:100] Skipping cls/predictions/transform/LayerNorm/beta/adam_m\n",
      "I0731 22:02:06.167646 139833534199616 modeling_bert.py:100] Skipping cls/predictions/transform/LayerNorm/beta/adam_v\n",
      "FinBERT Converting: ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.167740 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\n",
      "I0731 22:02:06.167814 139833534199616 modeling_bert.py:100] Skipping cls/predictions/transform/LayerNorm/gamma/adam_m\n",
      "I0731 22:02:06.167876 139833534199616 modeling_bert.py:100] Skipping cls/predictions/transform/LayerNorm/gamma/adam_v\n",
      "FinBERT Converting: ['cls', 'predictions', 'transform', 'dense', 'bias']\n",
      "I0731 22:02:06.167969 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\n",
      "I0731 22:02:06.168041 139833534199616 modeling_bert.py:100] Skipping cls/predictions/transform/dense/bias/adam_m\n",
      "I0731 22:02:06.168102 139833534199616 modeling_bert.py:100] Skipping cls/predictions/transform/dense/bias/adam_v\n",
      "FinBERT Converting: ['cls', 'predictions', 'transform', 'dense', 'kernel']\n",
      "I0731 22:02:06.168200 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\n",
      "I0731 22:02:06.168466 139833534199616 modeling_bert.py:100] Skipping cls/predictions/transform/dense/kernel/adam_m\n",
      "I0731 22:02:06.168538 139833534199616 modeling_bert.py:100] Skipping cls/predictions/transform/dense/kernel/adam_v\n",
      "FinBERT Converting: ['cls', 'seq_relationship', 'output_bias']\n",
      "I0731 22:02:06.168631 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\n",
      "I0731 22:02:06.168707 139833534199616 modeling_bert.py:100] Skipping cls/seq_relationship/output_bias/adam_m\n",
      "I0731 22:02:06.168770 139833534199616 modeling_bert.py:100] Skipping cls/seq_relationship/output_bias/adam_v\n",
      "FinBERT Converting: ['cls', 'seq_relationship', 'output_weights']\n",
      "I0731 22:02:06.168858 139833534199616 modeling_bert.py:137] Initialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\n",
      "I0731 22:02:06.168930 139833534199616 modeling_bert.py:100] Skipping cls/seq_relationship/output_weights/adam_m\n",
      "I0731 22:02:06.168991 139833534199616 modeling_bert.py:100] Skipping cls/seq_relationship/output_weights/adam_v\n",
      "I0731 22:02:06.169045 139833534199616 modeling_bert.py:100] Skipping global_step\n",
      "I0731 22:02:06.169100 139833534199616 modeling_bert.py:100] Skipping good_steps\n",
      "I0731 22:02:06.169154 139833534199616 modeling_bert.py:100] Skipping loss_scale\n",
      "Save PyTorch model to FinBERT-Prime_128MSL-250K.bin\n"
     ]
    }
   ],
   "source": [
    "convert('gold/FinBERT-Prime_128MSL-250K/model.ckpt-250000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
